Survey
A SANS 2021 Survey: Vulnerability Management-- Impacts on Cloud and the Remote Workforce
Written by David Hazar November 2021

©2021 SANSTM Institute

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

1

Executive Summary
Vulnerability management (VM) continues to be a struggle for many organizations. As we observed last year, companies have been tracking vulnerabilities in their systems and third-party software since the late 1990s and shortly after had the ability to automatically identify vulnerabilities in their systems, software, and even custom-developed applications.1
However, even though many organizations have well-defined vulnerability management programs, there are certain aspects of these programs that continue to vex survey respondents and prevent their organizations from maturing.
Identifying most vulnerabilities is typically not very hard, but fixing vulnerabilities is difficult for a variety of reasons. Respondents listed these, among others:
· We don't budget for it--and we don't have extra time or resources.
· Operational teams are already overworked.
· It never ends. Even if we remediate everything, new vulnerabilities are constantly being discovered, and reports come in at different times and in different formats, depending on the tools or teams being leveraged for identification.
· It's a business expectation, but not always a business requirement; therefore, the effort is not always recognized and rewarded.
· Security is accountable--but not responsible--for much of the work.
To succeed with vulnerability management, it takes a coordinated effort among security, IT (both systems and software development), and the business operations groups. Organizations must also identify, acknowledge, and track the roadblocks and technical debt within the organization. Many times there are significant barriers that prevent timely remediation of vulnerabilities. It is not uncommon to analyze vulnerability backlogs and determine that well over 50% of the outstanding vulnerabilities cannot be remediated following normal treatment processes or with the operational budgets and resources currently allocated.
In this year's survey, we looked at some of the same measures we looked at in the previous two surveys. However, we also wanted to get more information on the responding organizations' maturity across the different phases of the VM life-cycle. To accomplish this, we asked respondents to rate themselves against the SANS Vulnerability Management Maturity Model, which addresses the following life-cycle phases and functions:
· Prepare - Policy and standards - Context
· Identify - Automated identification - Manual identification - External identification (security researchers and crowdsourced identification)

1 "SANS Vulnerability Management Survey 2020," www.sans.org/white-papers/39930/ [Registration required.]

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

2

· Analyze - Prioritization - Root cause analysis
· Communicate - Metrics and reporting - Alerting
· Treat - Change management - Patch management - Configuration management
We developed and released the SANS Vulnerability Management Maturity Model in late 2019 after many students of the class that SANS Certified Instructor Jonathan Risto and the writer of this paper co-authored asked what framework or standard they could use to measure their own maturity. Since then, we have also had many students of MGT516: Managing Security Vulnerabilities: Enterprise & Cloud ask for information about how they are doing compared with the industry or compared to their peers. So, we added the maturity model to the survey this year. We wanted to begin tracking this data so that it is available for organizations to provide that point of comparison.
Some of the key findings and takeaways from the survey include:
· The percentage of companies with a formal program continues to increase from 63% in 2020 to 75% in 2021 with the remaining participants either having an informal program or planning on creating a formal program in the next 12 months.
· An increase in cloud, container, and custom software development or application VM requirements and capabilities over levels reported in 20192 and 2020,3 accompanied by maturity across almost all life-cycle phases being comparatively lower for these asset types.
· In terms of roles and responsibilities, the data shows that IT is taking a larger role in running the overall VM program than in the past, but this difference could also be due to the demographics of this year's survey.
· More than half the respondents (68%) are at least at a defined level of maturity for their prioritization or risk ranking processes and procedures.
· Many organizations have a continued lack of confidence in the maturity of their configuration management capabilities, especially for container and cloud assets.
Because we conducted similar vulnerability management surveys in 2019 and 2020,4 we also analyzed some of the changes to determine what progress has been made and identify some of the year-over-year differences.

2 "SANS Vulnerability Management Survey," April 2019, www.sans.org/white-papers/38900
3 "SANS Vulnerability Management Survey," November 2020, www.sans.org/white-papers/39930
4 "SANS Vulnerability Management Survey," April 2019, www.sans.org/white-papers/38900 and "SANS Vulnerability Management Survey," November 2020, www.sans.org/white-papers/39930

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

3

Survey Demographics

As with the past couple of years, the majority of respondents came from organizations headquartered in North America, followed by Europe and Asia. Although 81% of respondents have operations in the United States, survey results still show a global presence--almost one-third of the respondents' organizations have operations in Canada, Europe, and Asia, and close to a quarter maintain operations in Australia/New Zealand and Latin/South America. The industries shifted a little bit from previous years, with government and technology organizations edging out respondents from financial services for the top spots followed by cybersecurity, education, healthcare, manufacturing, and retail. Small and midsize businesses had greater participation than in previous years, but companies with more than 10,000 employees accounted for 31% of the participants. Despite more participation from companies with a smaller size in terms of people, 53% of companies had a revenue of over $250 million.
Figure 1 provides a snapshot of the demographics for the respondents to the 2021 survey.

Top 4 Industries Represented
Government

Technology
Banking and finance
Cybersecurity
Each gear represents 5 respondents.
Operations and Headquarters

Ops: 46 HQ: 9

Ops: 49 HQ: 13

Ops: 46 HQ: 6

Ops: 121 HQ: 112
Ops: 34 HQ: 3

Ops: 23 HQ: 2

Ops: 23 HQ: 1

Ops: 37 HQ: 3

Organizational Size
Small
(Up to 1,000)
Small/Medium
(1,001­5,000)
Medium
(5,001­15,000)
Medium/Large
(15,001­50,000)
Large
(More than 50,000) Each building represents 5 respondents.
Top 4 Roles Represented
Security administrator/ Security analyst Security manager or director
Security architect
IT manager or director
Each person represents 5 respondents.
Figure 1. Key Demographic Information

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

4

Setting the Stage

It was great to learn that the percentage of organizations with formal programs managed either internally (63%) or through a third party (11%) is up more than 11 percentage points from last year and almost 20 points from 2019. The majority of those that do not have a formal program are still informally managing their vulnerabilities (18%) in some fashion, while the remainder have plans to formalize a program in the next 12 months (7%). This is the first year that no respondents indicated they did not have any program and did not plan to have one. See Figure 2.
These results indicate that more than 92% of organizations have at least some processes in place to identify or manage their vulnerabilities. As expected, the larger the organization, the more likely it is to have a formal program (see Table 1). The industries most likely to have a formal program are financial services and government.

Does your organization have a vulnerability management program?

7.4% 18.1% 11.4%

63.1%

Yes, we have a formal program managed internally
Yes, we have a formal program managed by a third party
Yes, we have an informal program
No, we do not have a program, but we plan to in the next 12 months

Figure 2. Formal vs. Informal Programs


Table 1. Formal Versus Informal Programs by Organization Size

Does your organization have a vulnerability
management program?

Total Count
Yes, we have a formal program managed internally.
Yes, we have a formal program managed by a third party.
Yes, we have an informal program.
No, we do not have a program, but we plan to in the next 12 months.
No, we do not have a program and don't plan to.

Fewer than 101­

Total

100

500

100.0% 100.0% 100.0%

501­ 1,000 100.0%

Organization Size

1,001­ 2,000

2,001­ 5,000

5,001­ 10,000

100.0% 100.0% 100.0%

10,001­ 15,000 100.0%

15,001­ 50,000 100.0%

50,001­ More than 100,000 100,000 100.0% 100.0%

62.3% 69.2% 52.0% 60.9% 57.9% 46.2% 72.7% 60.0% 78.6% 60.0% 76.9%

11.3% 17.9% 7.3%

0.0% 7.7% 23.1%

32.0% 12.0% 4.0%

8.7% 21.7% 4.3%

10.5% 21.1% 10.5%

0.0% 38.5% 15.4%

9.1% 18.2% 0.0%

13.3% 20.0%
6.7%

14.3% 0.0% 0.0%

0.0% 20.0% 20.0%

0.0% 23.1% 0.0%

1.3% 0.0% 0.0% 4.3% 0.0% 0.0% 0.0% 0.0%

7.1% 0.0% 0.0%

Respondents also indicated the specific types of assets and functions that they included or planned to include in their vulnerability management program. Not surprisingly, infrastructure is still the main focus, with on-premises infrastructure being included by the most organizations (95%) and various cloud services making a strong showing. See Figure 3 on the next page.

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

5

Reviewing the 2021 results against those from 2020, we saw big increases to the assets that were existing or planned to be part of the VM program in almost every category, with the exception of traditional on-premises infrastructure, which had only a small increase. This could be attributed to the fact that the participation was already high, but it also may align with a shift away from traditional operating models.
Responsibility for Vulnerability Management Programs
Information security is still the most common group assigned responsibility for overall organizational vulnerability management (63%), but IT was responsible for VM in a greater number of organizations this year. Respondents continue to indicate that a lot of responsibility is placed on IT organizations for remediation activities such as patch (63%) and configuration management (65%), as illustrated in Table 2. Manufacturing and retail were the industries most likely to respond that overall vulnerability management was an IT responsibility. Audit, risk, and compliance are still more heavily involved in application vulnerability management than other asset types. They are most involved in vulnerability analysis and reporting, which may be due to their primary focus being on the business and its associated risks.

Which are included as part of your existing or planned vulnerability management program? Select all that apply.

Existing
On-premises traditional (physical/ virtual) infrastructure (servers, endpoints,
network devices, appliances, etc.)
Asset inventory tools

Planned 10.4%
20.0%

84.3% 75.7%

Ticketing systems

16.5%

73.9%

Threat intelligence

25.2%

64.3%

Cloud Software-as-a-Service (SaaS)

60.9% 27.0%

Cloud Infrastructure-as-a-Service (IaaS)

56.5% 27.8%

Custom software or application development (Internal)

53.0% 27.0%

Third-party/open source developed applications (not packaged software)

53.0% 24.3%

Cloud Platform-as-a-Service (PaaS)

49.6% 31.3%

IoT/embedded/industrial control system (ICS) infrastructure

41.7% 31.3%

Container infrastructure

38.3% 39.1%

Other

9.6% 8.7%

0%

20% 40% 60% 80% 100%

Figure 3. Vulnerability Management Program Assets

Table 2. Primary Responsibility

Overall vulnerability management in your organization Vulnerability reporting Vulnerability analysis Traditional (physical/virtual) infrastructure vulnerability discovery Cloud vulnerability discovery Third-party/open source application vulnerability discovery Custom-developed application vulnerability discovery Container infrastructure vulnerability discovery IoT/embedded/ICS vulnerability discovery Patch management Configuration management

Information Information Application Security Technology Development Audit/Risk

63.4%

23.2%

2.7%

4.5%

60.7%

11.6%

4.5%

8.0%

59.8%

13.4%

7.1%

4.5%

50.0%

31.3%

3.6%

3.6%

47.3%

13.4%

6.3%

3.6%

47.3%

11.6%

8.9%

7.1%

43.8%

18.8%

14.3%

5.4%

40.2%

17.0%

6.3%

6.3%

34.8%

13.4%

8.0%

5.4%

16.1%

63.4%

6.3%

1.8%

12.5%

65.2%

5.4%

5.4%

Compliance 1.8% 2.7% 3.6% 2.7% 3.6% 2.7% 0.9% 3.6% 0.9% 3.6% 3.6%

Third Party 0.9% 1.8% 3.6% 0.9% 4.5% 5.4% 0.9% 0.9% 4.5% 3.6% 2.7%

Other 2.7% 4.5% 2.7% 4.5% 3.6% 2.7% 0.9% 1.8% 2.7% 1.8% 1.8%

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

6

Automated vulnerability discovery increased by 10 percentage points to 81% of respondents, but that does not ensure that all assets in a given category are subject to automated scanning. Traditional, on-premises infrastructure continues to lead other asset types in being automatically assessed for vulnerabilities at 74%, with all others selected by fewer than 35% of respondents. We continue to see the least amount of automated discovery happening for applications and IoT/embedded systems. See Figure 4.

Which of the following are automated? Select all that apply.

On-premises traditional (physical/ virtual) infrastructure (servers, endpoints,
network devices, appliances, etc.)
Cloud infrastructure-as-a-service

73.5% 34.9%

Cloud platform-as-a-service

26.5%

Container infrastructure

25.3%

Cloud Software-as-a-Service

25.3%

IoT/embedded/ICS infrastructure
Third-party/open source developed applications (not packaged software)
Custom software or application development (internal)
Other 2.4%
0%

24.1% 21.7% 16.9%

20%

40%

60%

80%

Figure 4. Automated Discovery by Asset Type

The numbers for IoT/embedded/ICS systems might be due to the fact that many organizations are using their traditional infrastructure scanning technologies in this space. Although the lower percentages for some of these asset types are somewhat surprising, it is important to recognize that organizations could still be using manual forms of identification and relying on patch and configuration management tools to notify them of outdated software or insecure configurations.

Although a smaller number of respondents are managing business partner vulnerabilities compared with the 2019 survey, how these business partners are assessed has not changed drastically. It does, however, seem that more businesses are comfortable asking for access to scan for vulnerabilities in their partners' environments. See Figure 5.

How does your organization manage business partner vulnerabilities? Select all that apply.

Business partners provide us vulnerability information from their internal processes.
We require business partners to allow us to scan their applications for vulnerabilities.
Business partners are required to provide an attestation from a third party that they comply with certain vulnerability management
requirements that we have defined.
We require business partners to allow us to scan their infrastructure for vulnerabilities.
We perform an initial assessment of the business partner's infrastructure and/ or applications during negotiations.

Other 2.5%

0%

10%

50.0% 47.5% 40.0% 32.5% 25.0%

20%

30%

40%

50%

Figure 5. Partner Management

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

7

VM Maturity

This is the first year we have asked respondents to rate their maturity based on the SANS VM Maturity Model. It will be interesting to see how maturity changes and the different trends in future surveys.

Prepare

Preparation is an important part of any program and it is not a one-time activity.

Many organizations have moved to more iterative styles of systems and software

development, and it is helpful to follow a similar iterative approach to program

development. Organizations cannot excel at everything right away. If they are

focusing on more than a few items

in each cycle, they will almost

How would you rank the maturity of your VM policies and standards?

certainly struggle to maintain

focus and make significant gains.
Policies and Standards
The maturity of respondents' policies and standards is almost a perfect bell curve with most organizations at a defined level of maturity. See Figure 6.
This means that a good number of organizations have started to mature past defined policies and standards to measuring compliance and, in some cases, leveraging automation to make compliance with policies and standards easier for the business.

Level 1: Policy and standards are undocumented
or in a state of change.
Level 2: Policy and standards are defined in specific areas as a result of a negative impact to the program, not based on a deliberate selection of best practices
or standards from recognized frameworks.
Level 3: Policy and standards have been carefully selected
based on best practices and recognized security frameworks and are updated as needed to fulfill the
program's mission. Employees are made aware of standards and training on requirements is available.
Level 4: Adherence to defined policy and standards is tracked
and deviations are highlighted. Training personnel on requirements is required at least annually.
Level 5: Automated, proactive controls enforce policy
and standards and provide input to regular updates and training requirements.
0%

Having defined policies and standards is essential to measuring our progress and

11.2% 21.5%

37.4%

20.6%

9.3%

10%

20%

30%

40%

Figure 6: Maturity of Policies and Standards

effectiveness, and setting clear expectations for auditors and other interested

third parties. If an organization can improve reporting and automate compliance

for some or many of the standards it has defined, it will reduce the burden for

program participants and allow them to focus on aspects of the program that are

more difficult to automate.

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

8

Context
When looking at the next category in the prepare phase of the VM life-cycle, it is not surprising that the maturity shifts lower. Many organizations continue to struggle to keep track of and manage their assets. See Figure 7.

How would you rank the maturity of your asset inventory and the contextual information you need as input to various VM processes?

Level 1:

Contextual data (e.g., asset details, ownership, relationships) is available from multiple data

17.8%

sources with varying degrees of accuracy.

Level 2: There is a central repository of contextual data that has some data for the majority of systems and applications.

30.8%

Although cloud and other types of programmable infrastructure may make querying our assets easier, the

Level 3: The central repository requires certain contextual information be tracked and updated for each system
and application, based on program needs.

assets are now much more dynamic

and may lack context if we are not appropriately leveraging tagging and other capabilities to store and leverage that context. Even if we are properly tagging our assets, our VM tools may struggle to leverage these tags and may not easily handle aging out assets that are more dynamic in

Level 4: Reports show compliance with contextual information
requirements, and processes are in place to identify non-compliant, missing, or retired systems and applications.
Level 5: Automated or technology-assisted processes and procedures exist to both create and remove systems
and applications and associated attributes from the central repository, or data is correlated and reconciled with other systems that contain information
about tracked systems and applications.
0%

nature. This is where tighter integration between our different tools can help--

whether it be integration between our asset management and VM tooling or

our programmable infrastructure and our asset management or VM tooling.

26.2%

17.8%

7.5%

10%

20%

30%

Figure 7. Maturity of Asset Inventory and Contextual Information

Identify
Identification is often how we define our vulnerability management programs. If there are automated tools in place to identify vulnerabilities, then we have a vulnerability management program. Although identification is a key part of vulnerability management, it does not solve the problem in a vacuum, which is why we cover so many different topics in the maturity model. Identification can happen in many different ways, but to simplify the model, there are three different functions we measure for maturity: automated identification, manual identification, and external identification.

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

9

Automated Identification

Automated identification of traditional infrastructure is by far the most mature area in this survey. See Figure 8.

How would you rank the maturity of your automated vulnerability identification capabilities across traditional infrastructure, applications,
containers, and cloud? Select a scale for each category.

Traditional infrastructure

Applications

Containers

Cloud

This makes a lot of sense, because we have had tools available to help us with this for decades and

Level 1: Infrastructure and applications are scanned ad-hoc or irregularly for vulnerability details, or vulnerability details are acquired from existing data repositories
or from the systems themselves as time permits.

there are many vendors in this space. Where companies seem to be struggling the most is in implementing similar capabilities for containers and the cloud. Some of the reasons for the lack of maturity in these areas may be a combination of these being newer deployment and operational models for a lot of organizations and possibly a need for vendors

Level 2: The process, configuration, and schedule for scanning infrastructure and applications is defined and followed for certain departments or divisions within the organization; available technology may vary throughout the organization.
Level 3: There are defined and mandated organizationwide scanning
requirements and configurations for infrastructure and applications that set a minimum threshold for all departments
or divisions; technology is made available throughout the organization via enterprise licensing agreements or as a service.
Level 4: Scanning coverage is measured and includes the measurement
of authenticated vs. unauthenticated scanning (where applicable), the types of automated testing employed,
false positive rates, and vulnerability escape rates.

to improve their capabilities to assess and report on these types of resources. Although many

Level 5: Scanning is integrated into build and release processes and procedures and happens automatically in accordance
with requirements. Scanning configurations and rules are updated based on previous measurements.

of the traditional vulnerability

0%

management vendors are

capable of scanning in the cloud and include container scanning capabilities, these

capabilities are not always as mature or well-understood by consumers.

6.8% 14.6%

31.1% 30.1%

17.5% 15.5%

26.2% 29.1%

31.1% 25.2% 20.4% 22.3%

21.4% 21.4% 18.4% 15.5%

14.6% 6.8% 6.8%
9.7%

10%

20%

30%

Figure 8. Maturity of Automated Vulnerability Capabilities by Category

Surprisingly, automated vulnerability identification for applications is somewhere in the middle. The reason this is surprising is that we find that many companies struggle with application security or application vulnerability management much more than with their infrastructure due to either a lack of dedicated resources or an inadequate understanding of how to engage with development teams to drive remediation. We guess that many of the struggles in this area come after identification, which may be why maturity here is higher than expected.

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

10

Manual Identification
Manual identification maturity closely follows the trends we see in automated identification, but it is slightly less mature than automated identification across the board. See Figure 9.
Organizations have so much to monitor and assess these days that the trend is toward automation and away from manual processes. Nevertheless, it is important that organizations not ignore this function as certain application-layer vulnerabilities are not easily identified through automated identification technologies. Also, manual identification can provide much-needed data to justify time spent remediating identified vulnerabilities and can help organizations focus on the highest risk vulnerabilities in their backlogs.

How would you rank the maturity of your manual vulnerability identification capabilities across traditional infrastructure, applications, containers, and cloud?
Select a scale for each category.

Traditional infrastructure

Applications

Containers

Cloud

Level 1: Manual testing or review occurs when
specifically required or requested.

15.5% 21.4%

35.0% 32.0%

Level 2: Manual testing or review processes are established, and some departments and
divisions have defined requirements.
Level 3: Manual testing or review happens based on reasonable policy-defined requirements that apply to the entire organization and is available as a service where not specifically required by policy.
Level 4: Deviations from manual testing or review
requirements are tracked and reported.

23.3% 31.1%
17.5% 17.5%
29.1% 19.4%
21.4% 18.4%
20.4% 13.6%
14.6% 15.5%

Level 5: Manual testing or review processes include focused testing based on historical test data
and commonalities or threat intelligence.
0%

11.7% 10.7% 7.8%
11.7%

10%

20%

30%

40%

Figure 9. Maturity of Manual Vulnerability Capabilities by Category

External Identification
External identification may happen as part of a formal bug bounty program, but even if an organization does not have a bug bounty program, it needs to have a defined way of handling external vulnerability reports. Many of the respondents' organizations have definite room for growth in this area. See Figure 10.
The most important aspect of this function is to have and follow a defined vulnerability disclosure policy (VDP), but many companies have found that tapping into crowdsourced identification capabilities can be valuable. The researchers that are involved in this kind of work tend to be much more specialized and can provide significantly more rigorous testing within their area of focus.

How would you rank the maturity of your external vulnerability identification capabilities across traditional infrastructure, applications, containers, and cloud?
Select a scale for each category.

Traditional infrastructure

Applications

Containers

Cloud

Level 1: External vulnerability reports and disclosures
are handled on a case-by-case basis.

22.5% 24.5%

33.3% 34.3%

Level 2: Basic vulnerability disclosure policy (VDP) and contact information is published, but backend processes and procedures are not documented.
Level 3: More comprehensive VDP is in place, along with terms and conditions for external vendors and security researchers, that outlines rules of engagement, tracking, and feedback processes.
Level 4: Compliance with VDP and terms and conditions are tracked and measured. Information is used to streamline processes and evaluate vendors and researchers.
Level 5: A mature external testing and research program is in place with specific goals and campaigns that may only be available to specific vendors or researchers.
0%

22.5 21.6% 18.6% 13.7%

26.5% 24.5% 20.6% 22.5%

17.6% 13.7% 12.7%
15.7%

9.8% 12.7%
8.8% 6.9%

10%

20%

30%

40%

Figure 10. Maturity of External Vulnerability Capabilities by Category

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

11

Analyze
If organizations want to understand what is working or not working in their programs, they will spend a good amount of time analyzing the data. While much of the focus in the industry is on prioritization-- possibly due to the fact that it is easier to market a product that can successfully help you in this area--it is also important to dig into the details and analyze why certain metrics are not what we would hope for or expect. Why aren't teams patching patchable vulnerabilities? Why do certain technologies seem to consistently cause the most problems?

How would you rank the maturity of your prioritization or risk-ranking processes and procedures?

Level 1:

Prioritization is performed based on CVSS/ severity designations provided by identification

9.8%

technology or indicated in reports.

Level 2: Prioritization also includes analysis of other available fields, such as whether or not exploits
or malware exist, or confidence scores.
Level 3: Prioritization includes correlation with the affected asset, asset group, or application to account for its
criticality in addition to the severity designation. This may require light to moderate customization
depending on architecture and design.
Level 4: Generic threat intelligence or other custom data, which may require additional products or services, is leveraged to perform prioritization.
Level 5: Company-specific threat intelligence, or other information
gathered from the operating environment, is leveraged to preform prioritization. This information may require
human analysis or more extensive customization.
0%

22.5%

35.3%

20.6%

11.8%

10%

20%

30%

Figure 11. Maturity of Prioritization or Risk Ranking Processes

Prioritization
With all the industry talk and tooling around prioritization and risk scoring, it is not surprising that around 68% of respondents are defined Level 3 or better for maturity of their risk-ranking or prioritization procedures. See Figure 11.

Even though an organization's asset inventories may not be perfect, there is still value to

be gained from joining this data set with its vulnerabilities to allow for better prioritization

of the vulnerabilities. Layering threat intelligence on top of the other attributes the organization uses helps make the prioritization more relevant temporally.
Root Cause Analysis

How would you rank the maturity of your root cause analysis processes and procedures?

Level 1: Root cause analysis is performed based on out-of-the-box
information such as standard remediation/patch reports or other categorized reports (e.g., OWASP Top 10 category).
Level 2: Data is lightly customized to apply less granular or more meaningful groupings of data than CVE, CWE, or
Top 10 identifiers to facilitate root cause analysis.

17.8% 28.7%

What is a bit more surprising is that 54% self-select at Level 3 or better for root cause analysis. So, while there is a bit more focus on prioritization, over half the

Level 3: Data is also identified, grouped, and/or filtered by department or location to enable identification of location or group-based deficiencies. This may require light to moderate customization depending on architecture and design.
Level 4: Data is also identified, grouped, and/or filtered by owner or role. This may require more extensive
customization and ongoing maintenance.

22.8% 23.8%

respondents have a defined process

Level 5: Executive dashboard exists including highest-risk root

for looking into root cause issues as

cause impediments, exclusions, project cost projections, etc. This will require more detailed analysis and

6.9%

well. See Figure 12.

customization to make meaningful and should integrate with existing executive business intelligence (BI) tools.

0%

10%

20%

30%

Figure 12. Maturity of Root Cause Analysis

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

12

This is surprising because we find that many organizations struggle to adequately acknowledge and communicate problems that may require support from outside the program and participating technology organizations. This may show that the problem is not identifying the problems, but instead communicating them adequately or broadly enough to get support for change.

Communicate
Communication plays a key role in establishing buy-in from the community of VM stakeholders. Use cases can range from getting executives and board members to fund special projects that deal with technical debt to influencing IT and development stakeholders to engage more meaningfully in treating or remediating vulnerabilities. Making sure we are building and refining the right stories--backed by data--is a key component of this phase of the life cycle.
Metrics and Reporting
We need to understand what reports and metrics resonate with our audience, but reports and metrics alone do not always leave a lasting impression. See Figure 13.

How would you rank the maturity of your VM metrics and reporting?

Level 1:

Simple, point-in-time operational metrics are available primarily sourced from out-of-the-box (OOB) reports

11.7%

leveraging minimal customization or filtering.

Level 2: Filtered reports are created to target specific groups or prioritize findings. Specific divisions or departments have defined their own reporting requirements, including both program and operational metrics, and generate and release the corresponding reports at a defined interval.
Level 3: Reporting requirements, including all required program,
operational, and executive metrics and trends, are well-defined. Baseline reports are consistent throughout the organization and tailored or filtered
to the individual departments or stakeholders.
Level 4: Reports and metrices include an indication of compliance with defined policy and standards, treatment timelines, and bug bars. Correlation with other security or contextual data sources allows for more meaningful grouping, improves accuracy, and allows for identification of faulty or inefficient design patterns.
Level 5: Custom reporting is available as a service or via self-
service options or feedback is regularly solicited. Reports are updated to reflect changing needs.
Automated outlier and trend analysis along with exclusion tracking is performed to identify high/low performers and highlight systemic issues/successes.
0%

33.0% 24.3% 19.4%

11.7%

10%

20%

30%

Figure 13. Maturity of Metrics and Reporting

Alerting
We also need to make sure we are properly defining both the quantity and quality of the alerts we are sending out to our different stakeholders. Not everything is a fire and analysts will ignore alerts if the system generates too many. Still, the strategic use of alerts can nudge people in the right direction and help us respond to critical or emergency issues.

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

13

When looking at reporting, metrics, and alerting in terms of organizational maturity, it appears that organizations are a little more confident in their alerting capabilities than in the reports and metrics they have available to present to stakeholders. This is most likely due to the relatively poor selection of out-of-thebox reports and metrics and the difficulty of creating custom reports and metrics in many of the identification technologies. See Figure 14.

How would you rank the maturity of your VM alerting processes and procedures?

Level 1: Alerting is either not available or only available
within security-specific technologies.
Level 2: Integrations exist and alerts are sent for specific divisions
or departments or for users of specific non-security technologies already being leveraged by some stakeholders.
Level 3: Alerting is available for most stakeholders
in their technology of choice.
Level 4: Visibility and both timing and detail of response
to alerts is measured and tracked.
Level 5: Data is analyzed to develop standard or automated response to alerts for common issues that can be tied to a common response.
0%

19.0%

25.0%

25.0% 19.0%

12.0%

10%

20%

30%

Figure 14. Maturity of Alerting Processes and Procedures

With alerts, we can focus on certain critical issues, and it is easier to tailor the messaging and drive specific behaviors. With reports and metrics, if we have not put in the effort to target specific groups with specific vulnerabilities on which they have both the responsibility and capability to act, then chances they will not have the desired effect. The vulnerabilities that the responsible stakeholder can act on will get lost in a sea of vulnerabilities that they are either not responsible for or that cannot be solved with a simple patch, configuration change, or small update to the code.

Treat

Treatment or remediation is the end goal of any vulnerability management program. The

problem is that it is rarely the responsibility of a single team. Moreover, the responsible

parties are typically not directly responsible for vulnerability management. These

teams were likely not hired to remediate vulnerabilities. Instead,

they were hired to build product or engineer systems and supporting infrastructure. This is one of the reasons why robust analysis and consistent, clear, and simple communication are key to overall success.
While there is an expectation that we all care about security and that

If these stakeholders feel as if the organization is wasting their time or that the data being presented is suspect, it is easy for them to disengage and focus on what they were hired to do--which, again, is not vulnerability management.

it is important, there will always be competing priorities. Vulnerability

management program leaders need to find a way to balance the needs of the program

with those of the overall business. They also need to recognize when groups of

vulnerabilities are too difficult to resolve within the organization's existing architecture

or within the responsible group's existing budget or resources. Once these groups

are identified, the conversations need to be directed away from the engineering and

operations teams and toward the executives and board members who can approve

special projects or budget allocations to resolve the underlying roadblocks.

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

14

Change Management
As we looked at organizations' maturity as it relates to change management, containers and cloud seem to be the most immature, which mirrors most of the other phases and functions assessed. Traditional infrastructure and applications rate on the higher end, which makes sense as these areas were why we implemented change management in the first place. See Figure 15.

How would you rank the maturity of your change management processes and procedures as they relate to VM across traditional infrastructure, applications, containers, and cloud? Select a scale for each category.

Traditional infrastructure

Applications

Containers

Cloud

Level 1: Changes related to VM activities pass through
the same workflow as any other change.

15.7% 18.6%

28.4% 27.5%

Level 2: Some changes related to VM activities have a custom
workflow or are treated as standard changes.

23.5% 22.5% 20.6% 12.7%

Level 3: Most changes related to VM activities follow a custom
workflow or are treated as standard changes.

26.5% 30.4%
22.5% 26.5%

The struggle with containers and cloud is not only that they are newer technologies, but also that they do not integrate as easily into traditional change management practices

Level 4: Changes related to VM activities along with success rates are tracked. Timing is also measured for different stages of the change or subtasks related to the change.
Level 5: Metrics from VM change activities are used to modify requirements or streamline future change requests.
At least some standard changes are automated.

21.6% 13.7% 9.8% 12.7% 11.8% 10.8% 8.8% 11.8%

because of the dynamic nature

0%

10%

20%

30%

of these resources. Organizations

Figure 15. Maturity of Change Management by Category

need to spend time adapting their change processes

and procedures or determine how to qualify many of

the container and cloud changes as standard changes to

reduce the number of rigorous reviews.

Patch Management

How would you rank the overall maturity of your patch management processes?

Although we didn't measure the

Level 1:

maturity of patch management across

Patches are applied manually or scheduled

4.2%

by admins and end users.

all categories, we would assume that the process would rank more mature

Level 2: There is a standard schedule defined and technology is available for some divisions or departments or for some

than configuration management

platforms to automate patch testing and deployment.

for most organizations across most

Level 3: All departments are required to patch within a certain

asset types based on the reduced

timeframe. Technologies are available to assist with testing and applying patches for all approved platforms.

complexity associated with setting up and managing patches in comparison

Level 4: Patch management activities are tracked along with compliance with remediation timelines and success rate.

29.2% 25.0%
33.3%

to configurations. See Figure 16.
Keep in mind, however, that we are measuring the maturity of the

Level 5: Data from patch management activities, security incidents,
and threat intelligence is used to right-size remediation timelines and identify process or technology changes.
0%

8.3%

10%

20%

30%

organizations' treatment processes.

Figure 16. Maturity of Overall Patch Management

This does not account for the possibility there are obstacles

that cause patches and their associated vulnerabilities from

being excluded from the regular process. Our processes may

still be mature even if our ability to execute those processes

may be less mature for specific types of vulnerabilities.

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

15

Configuration Management
For configuration management, we see a similar picture. See Figure 17. It is surprising that containers do not rank as more mature in this category. Container architecture lends itself to more fixed, immutable configurations given that most containers are not designed to support change once they are running. This would seem to simplify configuration management and virtually eliminate drift.
However, it is possible respondents are struggling to manage and assess their container images and therefore do not have confidence in the configuration of these assets.
Cloud Vulnerability Management

How would you rank the maturity of your configuration management processes and procedures as they relate to VM across traditional infrastructure, applications, containers, and cloud? Select a scale for each category.

Traditional infrastructure

Applications

Containers

Cloud

Level 1: Configuration requirements are not well-defined
and changes are either applied manually or the automatic application of configurations
is only available for a subset of platforms.

15.0% 20.4%

32.6% 28.1%

Level 2: Configurations are defined for some divisions
or departments or for specific platforms.

29.0% 24.5%
26.3% 29.2%

Level 3: Configurations are defined for all supported platforms. Technologies are available to automate or validate configuration changes for all platforms.

16.8% 19.8%

29.0% 25.5%

Level 4: Deviations from configuration requirements and associated service impacts are measured and tracked.

15.0% 21.4%
14.7% 14.6%

Level 5: Data from the configuration process along with security incidents and threat intelligence is leveraged to strengthen
or relax requirements as needed.
0%

12.0% 8.2%
9.5% 8.3%

10%

20%

30%

Figure 17. Maturity of Configuration Management by Category

The last area of maturity we covered was how well respondents

felt their organization was doing at managing vulnerabilities in the cloud. Just over 50% of the organizations rated themselves as falling within Levels 1 or 2. This is not surprising, given the fact that we are still struggling with traditional infrastructure

Cloud and container architectures can help organizations reduce the workload associated with managing configurations, especially for platform- and software-as-a-service or serverless infrastructure.

even years after implementation--and the cloud adds layers of

complexity, scale, and change on top of everything else. See Figure 18.
SANS thinks that there is a huge opportunity to do better VM in the cloud, but it will take careful planning and design to ensure the scale and rate of change do not wipe away

How would you rank the overall maturity of your VM program in addressing vulnerabilities in the cloud?
Level 1: Cloud infrastructure and applications are managed
the same as on-premises technologies.
Level 2: Some modifications have been made to processes to account for cloud architecture and design differences. Some cloud management technologies are being leveraged

28.0% 23.0%

these benefits. To be successful, organizations need to have a strong understanding of the shared responsibility model, where the responsibility of the cloud provider

Level 3: All processes have been analyzed and, where needed, tailored for the cloud. Cloud management technologies
are broadly leveraged to account for cloud risks.
Level 4: Metrics, alerts, and reports include cloudspecific data and risks as well as compliance
with cloud-specific requirements.

23.0% 16.0%

ends and the organizations begins, as well as what cloud-native or other technologies are available to help them succeed.

Level 5: Data from cloud monitoring is used to update images and code used to provision resources and applications in the cloud.
0%

9.0%

10%

20%

30%

Figure 18. Maturity of Cloud VM Overall

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

16

Summary and Final Recommendations
Most organizations are trying their best to manage vulnerabilities. Typically, however, organizations face challenging and expensive problems that are preventing certain types of vulnerabilities from being addressed. Organizations need to highlight these obstacles, communicate them effectively, and justify the funding and support required to remove these obstacles if they wish to succeed. As we continue to move away from traditional on-premises infrastructure toward containerized or cloud operating models, we need to take advantage of any opportunities that help us avoid similar obstacles in the future. Almost any move from one environment to another will, at least temporarily, reduce the number of vulnerabilities. If organizations are not thoughtful in building in solutions to the common issues we are experiencing today, it won't be long before it ends up right back where it started or in even worse shape due to the ease with which its footprint can grow and expand.
By reviewing the maturity information provided in this survey, organizations can quickly see how their current programs compare with others and where they might want to focus. It can also help organizations understand where it might take more time or effort to mature. Chances are, if most organizations are struggling with maturity for a certain function, it is probably because those functions take quite a bit more time and effort. That doesn't necessarily mean it is an area that we should avoid, but it can help us make more informed decisions about short-, medium-, and long-term road maps for maturing our programs. There is no quick fix to vulnerability management. Organizations need to incrementally and thoughtfully mature over time to succeed.


A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

17

About the Author
David Hazar is a SANS analyst, instructor, and co-author of SANS MGT516: Managing Security Vulnerabilities: Enterprise and Cloud. He is also an instructor and contributor for SANS SEC540: Cloud Security and DevOps Automation. A security consultant based in Salt Lake City, Utah, David focuses on vulnerability management, application security, cloud security, and DevOps. David has 20+ years of broad, deep technical experience gained from a wide variety of IT functions held throughout his career, including: developer, server admin, network admin, domain admin, telephony admin, database admin/developer, security engineer, risk manager, and AppSec engineer. He holds the CISSP, GWAPT, GWEB, GMOB, GCIA, GCIH, GCUX, GCWN, GSSP-.NET, and GSTRT certifications.
Sponsor
SANS would like to thank this survey's sponsor:

A SANS 2021 Survey: Vulnerability Management--Impacts on Cloud and the Remote Workforce

18

