Four Machine Learning Techniques that Tackle Scale
(And Not Just By Increasing Accuracy)
Lindsey Lack Gigamon Applied Threat Research (ATR)

99.9%

© 2018 Gigamon. All rights reserved.

2

Actual: benign Actual: malicious

Predict: benign 998940 90

Predict: malicious 910 60

False discovery rate 0.938

Actual: benign Actual: malicious

Predict: benign 998860 10

Predict: malicious 990 140

False discovery rate 0.876

© 2018 Gigamon. All rights reserved.

3

99.99%

© 2018 Gigamon. All rights reserved.

4

Actual: benign Actual: malicious

Predict: benign 999759 9

Predict: malicious 91 141

False discovery rate 0.392

Actual: benign Actual: malicious

Predict: benign 999825 75

Predict: malicious 25 75

False discovery rate 0.250

© 2018 Gigamon. All rights reserved.

5

Intended Audience

Cybersecurity-to-Data Science Continuum

© 2018 Gigamon. All rights reserved.

6

Overview
Measurement Explainability Confidence Architecture

© 2018 Gigamon. All rights reserved.

7

Measurement

© 2018 Gigamon. All rights reserved.

8

Actual: benign Actual: malicious

Predict: benign 999759 9

Predict: malicious 91 141

Precision 0.608 Recall 0.940

Actual: benign Actual: malicious

Predict: benign 999825 75

Predict: malicious 25 75

Precision 0.750 Recall 0.500

© 2018 Gigamon. All rights reserved.

9

ROC Curve
ER Area

Image: scikit-learn.org

Cons: · Requires fine-grained output ("proba") · AUC changes not intuitive
Pros: · Visualize FP and FN tradeoffs · Allows for focus on false positives
· (Early Retrieval area)

© 2018 Gigamon. All rights reserved.

10

ROC Curves with Highlighted FPR

Image: Endgame

Full features

Reduced features

© 2018 Gigamon. All rights reserved.

11

Measurement
Takeaway
Measure items in a way that is consistent with operational goals. For security use cases, this may entail a focus on false positives.

© 2018 Gigamon. All rights reserved.

12

Explainability
AKA Interpretability

© 2018 Gigamon. All rights reserved.

13

The black box issue is in the news

Image: Washinton Times

Image: Information Age Image: TNW

Image: DARPA

© 2018 Gigamon. All rights reserved.

14

Model Explanations
The Basics
Built-In Feature Ranking
Valid for whole model only, does not inform for particular instance
Train and Examine Simpler Model
E.g. Decision tree

© 2018 Gigamon. All rights reserved.

15

Model Explanations
Recent Purpose-built Tools
LIME
2016 Local Interpretable Model-Agnostic Explanations

SHAP
2017 SHapley Additive exPlanation (SHAP) Values An extension of the Shapley values method Uses game theory and notion of fair `payouts'

Image: LIME Image: SHAP

© 2018 Gigamon. All rights reserved.

16

Model Explanations
Endgame SHAP analysis of Ember model

Image: Endgame blog

Image: Endgame blog

© 2018 Gigamon. All rights reserved.

17

Model Explanations
Use of trained layers for effective Nearest Neighbor calculations
Use trained penultimate layer output as instance vector for distance calculation

Malicious URI and Nearest Neighbors in Training Set

Distance URI

www.0576pet.cn/view-13285-1.html

0.094

www.0817auto.cn/view-12858701.html

0.144

mythproductionhouse.com/pre-win-error-page-al...

0.158

www.tamizhtube.com/search/label/\xe0\xae\xb5\...

© 2018 Gigamon. All rights reserved.

18

Model Explanations
Images: Basic technique occlusion mapping

Image: Selvaraju et al

© 2018 Gigamon. All rights reserved.

19

Model Explanations
Occlusion-based Analysis of Malicious RTF files

Credit: Zeiler et al, 2014

© 2018 Gigamon. All rights reserved.

20

Explainability
Takeaway
Don't settle for solitary outputs (label or single probability) from models. Provide model context or insight (many methods available) that allows an analyst to "scan" results.

© 2018 Gigamon. All rights reserved.

21

Confidence

© 2018 Gigamon. All rights reserved.

22

Model probability output !=
confidence

© 2018 Gigamon. All rights reserved.

23

Most entertaining example of confidence mismatch:
"Passing a Chicken through an MNIST Model" blog entry by Emilien Dupont

Image: emiliendupont blog

Source: https://emiliendupont.github.io/2018/03/14/mnist-chicken/

© 2018 Gigamon. All rights reserved. 24

Confidence Mismatch

Image: emiliendupont blog

Image: emiliendupont blog

Source: https://emiliendupont.github.io/2018/03/14/mnist-chicken/

© 2018 Gigamon. All rights reserved.

25

Use Autoencoder to identify training distribution

Image: Wikipedia, Chervinskii

Image: emiliendupont blog

Source: Chervinskii / CC BY-SA 4.0 Source: https://emiliendupont.github.io/2018/03/14/mnist-chicken/

© 2018 Gigamon. All rights reserved.

26

Finding Out-of-distribution Samples

Image: emiliendupont blog

Source: https://emiliendupont.github.io/2018/03/14/mnist-chicken/

© 2018 Gigamon. All rights reserved.

27

Separation of New Distribution (fashion) from Training Distribution

VAE reconstruction error

© 2018 Gigamon. All rights reserved.

28

Recent Work on Uncertainty (Harang et al)
Harang, Richard, and Ethan M. Rudd. "Principled Uncertainty Estimation for Deep Neural Networks." arXiv preprint arXiv:1810.12278 (2018).
Leverages Real-NVP
Real Non-Volumetric Preserving Transformations Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. "Density estimation using Real NVP." arXiv preprint arXiv:1605.08803 (2016).
Captures both class-conditional densities as well as overall density (and hence overall uncertainty)

Image: Harang et al

© 2018 Gigamon. All rights reserved.

29

Recent Work on Uncertainty, Continued (Harang et al)
Improvement in model performance based on removing samples with high uncertainty (dotted line)
Especially effective at low false positive rates!

Real-NVP based selection

Raw NN based selection

Image: Harang et al

© 2018 Gigamon. All rights reserved.

30

Enablers: PPLs (Probabalistic Programming Languages)

Automatic Differentiation Libraries

© 2018 Gigamon. All rights reserved.

31

Confidence
Takeaway
Consider using autoencoding or similar technique to detect (and annotate) outside-of-training-distribution samples. Keep an eye on probabilistic programming usability improvements. When suitable, adopt to improve models and provide valuable context about uncertainty.

© 2018 Gigamon. All rights reserved.

32

Architecture

© 2018 Gigamon. All rights reserved.

33

PE File

size < 10MB &&
!signed

benign malicious

© 2018 Gigamon. All rights reserved.

34

PE File PE File PE File

size < 10MB &&
!signed
benign malicious
benign
malicious

benign malicious

benign malicious

© 2018 Gigamon. All rights reserved.

35

Model Compression
Improves model size (memory), energy usage, speed, and sometimes accuracy Can replace other models or be integrated in multistage

© 2018 Gigamon. All rights reserved.

36

Image: Gholami et al

Ember
3 hidden layers
256 nodes

Objective:
Develop distributed malicious file classifier that minimizes costs while maintaining overall accuracy
Constraints:
 Limit considered costs to: parsing, model runtime, transport
 Use Ember dataset
 Use standardized model architecture: DNN, fully connected 3 hidden layers, 256 nodes per hidden layer

© 2018 Gigamon. All rights reserved.

37

Technique Selection
Q-Learning Network Important approach: jointly modeling cost and performance Implemented Janisch et al 2017, "Classification with Costly Features using Deep Reinforcement Learning" Reinforcement learning approach with DNN as agent "brain"
Double Deep

Image: Janisch et al

Image: Deepmind

© 2018 Gigamon. All rights reserved. 38

DDQN Results

0.1% decrease in accuracy < 1/5 the cost

© 2018 Gigamon. All rights reserved.

39

Architecture
Takeaway
Architectural improvements can apply directly to scaling. If you are missing data due to heuristic filtering techniques used at the edge, consider expanding the scope of modeling efforts to the edge.

© 2018 Gigamon. All rights reserved.

40

Review
Measurement Explainability Confidence Architecture

© 2018 Gigamon. All rights reserved.

41

Questions?

© 2018 Gigamon. All rights reserved.

42

References
Alber, Maximilian, Sebastian Lapuschkin, Philipp Seegerer, Miriam Hägele, Kristof T. Schütt, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller, Sven Dähne, and Pieter-Jan Kindermans. "iNNvestigate neural networks!." arXiv preprint arXiv:1808.04260 (2018). Anderson, Hyrum S., and Phil Roth. "EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models." arXiv preprint arXiv:1804.04637 (2018). Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. "Density estimation using Real NVP." arXiv preprint arXiv:1605.08803 (2016). Gholami, Amir, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and Kurt Keutzer. "SqueezeNext: Hardware-Aware Neural Network Design." arXiv preprint arXiv:1803.10615 (2018). Harang, Richard, and Ethan M. Rudd. "Principled Uncertainty Estimation for Deep Neural Networks." arXiv preprint arXiv:1810.12278 (2018). Janisch, Jaromír, Tomás Pevný, and Viliam Lisý. "Classification with Costly Features using Deep Reinforcement Learning." arXiv preprint arXiv:1711.07364 (2017). Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." In Advances in Neural Information Processing Systems, pp. 4765-4774. 2017. Pedregosa, Fabian, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel et al. "Scikit-learn: Machine learning in Python." Journal of machine learning research 12, no. Oct (2011): 2825-2830. Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization." In ICCV, pp. 618-626. 2017. Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144. ACM, 2016. Zeiler, Matthew D., and Rob Fergus. "Visualizing and understanding convolutional networks." In European conference on computer vision, pp. 818-833. Springer, Cham, 2014.
© 2018 Gigamon. All rights reserved. 43

References (Online)
https://www.endgame.com/blog/technical-blog/opening-machine-learning-black-box-model-interpretability https://github.com/albermax/innvestigate https://emiliendupont.github.io/2018/03/14/mnist-chicken/
© 2018 Gigamon. All rights reserved. 44

Image References

Image: scikit-learn.org Image: Endgame Image: Information Age Image: DARPA Image: Washington Times Image: TNW Image: LIME Image: SHAP Image: albermax Image: Selvaraju et al
Image: Endgame blog Image: emiliendupont blog Image: Wikipedia, Chervinskii Image: Harang et al
Image: Google Image: Gholami et al
Image: Deepmind Image: Janish et al

https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py https://github.com/endgameinc/ember/blob/master/resources/ember-notebook.ipynb https://www.information-age.com/explainable-ai-123476397/ https://www.darpa.mil/program/explainable-artificial-intelligence
https://www.washingtontimes.com/news/2018/jun/29/darpas-explainable-ai-a-common-sense-comfort-in-a-/
https://thenextweb.com/artificial-intelligence/2018/02/27/bye-bye-black-box-researchers-teach-ai-to-explain-itself/
https://homes.cs.washington.edu/~marcotcr/blog/lime/ https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20LightGBM.html https://github.com/albermax/innvestigate Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization." In ICCV, pp. 618-626. 2017. https://www.endgame.com/blog/technical-blog/opening-machine-learning-black-box-model-interpretability https://emiliendupont.github.io/2018/03/14/mnist-chicken/ https://en.wikipedia.org/wiki/Autoencoder
https://arxiv.org/abs/1810.12278 https://www.camlis.org/richard-harang https://blog.google/products/google-vr/google-lens-real-time-answers-questions-about-world-around-you/ Gholami, Amir, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and Kurt Keutzer. "SqueezeNext: Hardware-Aware Neural Network Design." arXiv preprint arXiv:1803.10615 (2018). https://deepmind.com/blog/ Janisch, Jaromír, Tomás Pevný, and Viliam Lisý. "Classification with Costly Features using Deep Reinforcement Learning." arXiv preprint arXiv:1711.07364 (2017).

© 2018 Gigamon. All rights reserved.

45

