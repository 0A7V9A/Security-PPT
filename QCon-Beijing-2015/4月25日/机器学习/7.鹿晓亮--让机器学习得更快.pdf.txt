
 


   











1920RadioRex 1950Bell Lab Audry 6-70DSPDTWViterbiHMMDARPA 1980DARPANISTSphinx 1990HTK 2000



      



HMM



























DNN

RNN

LSTM


20101028"" 









20125 DNN

86.2%

95%+
20137 DNN 


20111 

75.3%

20118
81.2% 


60.2%

20101028 

90%



 DeepID3 Face++ DeepID2+ DeepID2 DeepID DeepFace-ensemble FR+FCN GaussianFace Betaface.com TL JointBayesian


 DeepLearning


 99.53% 99.50% 99.47% 99.15% 97.45% 97.35% 96.45% 98.52% 98.08% 96.33% 99.20%


   


·   ""
·   
·   

CPU







 









  

 

Ngra m

......





Linux &  & 

CPUs4TB & 6TB10Gb/s

GMM-HMM

GPU

   


 DNN

RNN

CNN

......

 





Linux &  & 

GPGPUsIB



Deep LearningDNNRNN

CNN



 DNN

RNN

CNN

HMM

GMM

......

   



Linux &  & 

CPUs10Gb/s

GPGPUsIB

 
 







HPC&BigData 


   


 Acoustic model
DNN-HMM VS GMM-HMM
 Computation of DNN in SR
model parameters : more than tens of millions speech corpus: more than ten thousand of hours
 Acceleration
CPU ­ GPU ­ GPUs



training corpus
Fig. 2 Model parallelism

training corpus
Fig. 3 Data parallelism

Tradeoff between Speed-up and Convergence

SGD

Central Node

 central node, high bandwidth requirement

 conflict between model latency

GPU0

GPU1

GPU2

GPU3

and efficiency

training corpus
Fig. 4 ASGD applied to multiGPU in a server [4][6]



 GPU0

 



Training

corpus

GPU1

GPU3

GPU2

Fig. 5 Ring structure parallel strategy for multiple GPUs

 get mini-batch from training corpus
 receive the model from the previous node, and merge the local gradient to generate a new model
 send the new model to the next node and train the next mini-batch simultaneously



GPU0 start point

GPU1

GPU2

GPU3

residual residual

merge time

 asynchronous mode

transmit

time  no central node, one

train time

transmission per mini-batch

for each node, low bandwidth

requirement

 easy to hide transmission

...

and so on

...

Fig. 6 Timing analysis of the RSPS



GPU0 start point

GPU1

GPU2

GPU3

residual

merge time

overlap of transmission and

transmit time

computation

train
time Tresidual  Tcalc [nTtransmit  (n1)Tmerge ]  0

n(Ttransmit  Tmerge )  Tcalc +Tmerge

residual

...

and so on

...

Fig. 6 Timing analysis of the RSPS

n  Tcalc  Tmerge T  T transmit merge



Twait  max{Tresidual , 0} max{nTtransmit  (n 1)Tmerge  Tcalc , 0}

Speedup  Tsingle  n(Tcalc  Tmerge ) Tmultipe Tcalc  Tmerge  Twait

n

Speedup



 Tcalc

 Tmerge

 Ttransmit  Tmerge

if

n



Tcalc  Tmerge T  T transmit merge

 



else

 



Speedupmax



Tcalc  Tmerge T  T transmit merge

· Tcalc (larger mini-batch, eg. rectified linear units) · Ttransmit (compress transmission data, eg. quantize the gradient) · Tmerge (overlap merging, eg. pipelining, hierarchical merging)



7

6

5

speedup

4

3

2

1

0

0

1

2

3

4

5

6

7

8

9

the number of GPUs

Fig. 7 Relationship between the speedup and the number of GPUs


   



9,634,057



......



16410 

13.5404











2069.3 

 38°56'

3.1525





116°20'

 38°53





1.26



377835 

9,629,091  



 35°44'

 2188 140°50'




178

77°02' 

· 
­   
­   1000
­  



·  ·  · 



·  ·  · 

THANK YOU!

