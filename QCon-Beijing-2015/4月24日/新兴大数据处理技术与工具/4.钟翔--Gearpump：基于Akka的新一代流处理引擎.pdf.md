QCON Beijing 2015

Gearpump Akka


Mail: xiang.zhong@intel.com Weibohttp://weibo.com/clockfly

Intel | Intel  Big Data Technology Department

2015/4/24

What is Gearpump
· Akka based lightweight Real time data processing platform.
· Apache License http://gearpump.io

Simple and Powerful Message level streaming Long running daemons

· Akka: · Communication, concurrency, Isolation, and fault-tolerant

What is Akka? 2

What is Akka?
· Micro-service(Actor) oriented.
· Message Driven · Lock-free · Location-transparent · Better performance · Fail in-dependently · Scales linearly
It is like our human society, driven by message

Gearpump in Big Data Stack

visualization
SQL Catalyst StreamSQL Impala batch
store

Cluster manager monitor/alert/notify
Cloudera Manager

Visualization & management

Data explore

Machine learning

stream

Graphx

Analytics

storm

Engine

Gearpump

Storage
4

Why another streaming platform?
· The requirements are not fully met. · We need a higher abstraction to build software!
5

Streaming requirements

· Michael Stonebraker, The 8 Requirements of

Real-Time Stream Processing (2006)

My summary











Flexible Volume Speed Accuracy Visual

Easy programing High

In-Stream Exactly-once WYSWYG

Any time

throughput

Zero latency Message

Any where

Scale linearly HA

loss/delay/out

Any size Any source Any use case

Responsive of order Predictable

Dynamic DAG

6

StreamSQL

Gearpump Highlights

performance

100% Akka

Throughput 11 million/s (*)

2ms Latency

function Exactly-once

Dynamic DAG

Out of Order Message

usability Flexible DSL

DAG Visualization

Internet of Thing

[*] on 4 nodes

7

USING GEARPUMP 
8

How to Submit an application

1. Submit a Jar

Master Workers

Master Workers

YARN
Master Workers

AppMaster 2. Create AppMaster
1 2

3 4

Master

Workers

AppMaster Executor Executor Executor

AppMaster

4. Report Executor to AppMast9er

DAG representation and API
DAG API Syntax: Graph(A~>B~>C~>D, B~>E~>D)

DAG

Processor

A

B

Shuffle

Processor

Processor E
C Processor

D Processor

10

DAG API Example - WordCount
val context = new ClientContext() val split = Processor[Split](splitParallism) val sum = Processor[Sum](sumParallism) val app = StreamApplication("wordCount", Graph(split ~> sum), UserConfig.empty) val appId = context.submit(app) context.close()
class Split(taskContext : TaskContext, conf: UserConfig) extends Task(taskContext, conf) { override def onNext(msg : Message) : Unit = { /* split the line */ }
}
class Sum (taskContext : TaskContext, conf: UserConfig) extends Task(taskContext, conf) { override def onNext(msg : Message) : Unit = {/* do aggregation on word*/}
}
11

WordCount with DSL API
val context = ClientContext() val app = new StreamApp("dsl", context) val data = "This is a good start, bingo!! bingo!!" app.fromCollection(data.lines)
// word => (word, count = 1) .flatMap(line => line.split("[\\s]+")).map((_, 1)) // (word, count1), (word, count2) => (word, count1 + count2) .groupByKey().sum.log
val appId = context.submit(app) context.close()
12

High level DSL API Details

Concepts
StreamApp Stream
OP Transformer

Description
OP(Operator) Graph
path of OP Graph
Transformation on Stream

Transformer Operators

flatMap

merge

map

groupBy

filter

process

reduce

OP Graph
source source

Processor Graph

merge

flatmap reduce

sink

flatmap

map

X join

Optimize
Co-locate

sink
Bottom-up optimization
13

DAG Visualization
DAG Page

Track global min-Clock of all message

DAG: · Node size reflect throughput · Edge width represents flow rate · Red node means something goes wrong

14

DAG Visualization
Processor Page

Skew analysis

Task throughput and latency

Executor JVM deployment 15

PERFORMANCE TEST 
16

Throughput and Latency
Throughput: 11 million message/second

Latency: 17ms on full load

SOL Shuffle test 32 tasks->32 tasks 4 nodes 10GbE 32 core E52680

17

Scalability
· Test run on 100 nodes and 3000 tasks · Gearpump performance scales:

100 nodes

18

Fault-Tolerance: Recovery time

91 worker nodes, 1000 tasks

Failure scenarios

Recovery time [*]

comment

Cluster Master node Down

0 s

Master HA take effect

Cluster Worker node down Message loss

~ 10 seconds ~ 300 ms

timeout detection take a long time
Still optimizing Target will be less than 10ms

Application AppMaster down ~ 10 seconds

timeout detection take a log

Test environment: [*]: Recovery time

91 worker nodes, 1000 tasks (We use 7 machines is the time interval between: a) failure happen b)

to all

simultaitme 9e1 worker nodes)
tasks in topology resume processing

d1at9a.

GEARPUMP INTERNAL 
20

Overview and general ideas

Minclock service

MinClock service track the min timestamp of all pending messages in the system now and future

Replayable Source

Message(timestamp)

DAG runtime
State

Every message in the system Has a application timestamp(message birth time)

 High Performance Streaming

Normal Flow 21

Overview and general ideas

Minclock service

clock pause at Tp

 Detect Message loss at Tp

 replay from Tp

 recover DAG executor process

Replayable

DAG runtime

Source

Message(timestamp)

State

Recovery Flow

Checkpoint Store
Exactly-once State can be reconstructed by message replay: 22

1. High performance streaming 2. Detect Message loss and other failures 3. DAG Executor Recovery 4. Clock Service, know when message is lost 5. Message replay from clock 6. Exactly-once, de-duplication
23

Actor Hierarchy
100% Actor: communication, concurrency, isolation, error handling
YARN

Client Hook in and query state

Master Cluster HA Design

As general service 24

Master HA
· Quorum () · Conflict free data types(CRDT) for consistency

WWWooorrkrkekeerrr

leader
Master

CRDT Data type example:

standby Master

State Gossip

Standby Master

Decentralized: No central meta server

25

High performance Messaging Layer

· Akka remote message has a big overhead, (sender + receiver address) · Reduce 95% overhead (400 bytes to ~20 bytes)

convert from short address

Sync with other executors
convert to short address

Effective batching

26

Effective batching
Network Idle: Flush as fast as we can Network Busy: Smart batching until the network is open again.
Network Bandwidth Doubled
For 100 byte per message
This feature is ported from Storm-297
27

Flow Control
Pass back-pressure level-by-level

TTaTasaskskk

TTaTasaskskk

TTaTasaskskk

TTaTasaskskk

Sliding window

Back-pressure

Another option(not used): big-loop-feedback flow control

28

1. High performance streaming 2. Detect Message loss and other failures 3. DAG Recovery 4. Clock Service, know when message is lost 5. Message replay from clock 6. Exactly-once, de-duplication
29

Failure Detection
For Message loss:  AckRequest and Ack

For JVM Crash, Network Down:  Actor Supervision
Master Failure
AppMaster Failure
Executor Failure
Task
30

1. High performance streaming 2. Detect Message loss and other failures 3. DAG Recovery 4. Clock Service, know when message is lost 5. Message replay from clock 6. Exactly-once, de-duplication
31

DAG Recovery: Quarantine and Recover

1. Message loss detected

AppMaster

Global clock service

Source

 error

Store

detected

Executor

Executor

Task Task

Task

Send message

32

DAG Recovery: Quarantine and Recover

2. Use dynamic session ID to fence zombies
AppMaster

Global clock service

Source

 error detected

Store

Executor

Executor

Task Task

Task

Fence zombie

33

DAG Recovery: Quarantine and Recover

3. Recover the executor JVM, replay message

Source

Replay
 error detected

AppMaster

Global clock service
Store

Executor

Executor

isolate zombie

Task Send message

Task
 Recover 34

1. High performance streaming 2. Detect Message loss and other failures 3. DAG Recovery 4. Clock Service, know when message is lost 5. Message replay from clock 6. Exactly-once, de-duplication
35

Global Clock Service ­ track application min-Clock (1)

Report task min-clock
Definition: Task min-clock is Minimum of (
min timestamp of pending-messages in current task Task min-Clock of all upstream tasks )

A

B

C

E

D
Min-Clock of D is min-clock of global

36

Global Clock Service ­ track application min-Clock (2)

Report task min-clock

One Task can have thousands upstream tasks, how to effectively track all of them?
Source
task task task

Definition: Task min-clock is Minimum of (
min timestamp of pending-messages in current task Task min-Clock of all upstream tasks )

Task 37

Global Clock Service ­ track application min-Clock (3)

One Task can have thousands upstream tasks, how to effectively track all of them?

Level Clock Ever incremental

A

1000 Later

Source
task task task

Implementation

B

800

C

E

600

D

400 Earlier

Task

38

1. High performance streaming 2. Detect Message loss and other failures 3. DAG Executor Recovery 4. Clock Service, know when message is lost 5. Message replay from clock 6. Exactly-once, de-duplication
39

Source-based Message Replay
Replay from the very-beginning source Source
Normal Flow 40

Source-based Message Replay
Replay from the very-beginning source
Source

Replay from offset Resolve offset

Recovery Flow

Global Clock Service

41

1. High performance streaming 2. Detect Message loss and other failures 3. DAG Executor Recovery 4. Clock Service, know when message is lost 5. Message replay from clock 6. Exactly-once, de-duplication
42

Exactly-once message processing

Key: Ensure State(t) only contains message(timestamp <= t)
DAG runtime
Checkpoint Store

How?
43

Exactly-once message processing
Checkpoint Flow 44

Exactly-once message processing
Recovery Flow 45

Dynamic DAG

Use Pub-Sub model

Maintain the correct min clock during transition

 Remove processor E

Un-subscribe E from B

E Un-subscribe D from E

A

B

C

D

F Subscribe C to F  Add processor F
46

UNIQUE USE CASES 
47

IOT Transparent Cloud
Target Problem Large gap between edge device with data center
Location transparent. Same programming model on IOT device
dag on device side
Data Center
log
48

Unified log ingestion and processing
Target Problem Distributed application is broken into many isolated pieces
Single Gearpump Application For All

Agent Agent Agent

Kafka queue DAG

Collect logs from different places

Query Visualization
49

Exactly-once: Financial use cases
Target Problem transactional exactly-once real-time streaming system Admin Portal
Billing System User
Model Bill
Streaming System
Service 50

Transformers: Dynamical DAG
Target Problem No existing way to manipulate the DAG on the fly
Manipulate the DAG on the fly
 Dynamic Attach  Dynamic Replace  Dynamic Remove

Add Sub Graph

Delete

B

51

Eve: Online Machine Learning
Target Problem ML train and predict online in real-time for decision support.
· Decide immediately based online learning result

Input sensor
Output

Learn
Predict Decide
52

Crowd sourcing
53

APPLICATION AND DEMO
54

Example1: Stock Drawdown Tracking

Publish Alerts

Query

More than 3000 index

Group by Crawlers Stock id

Drawdown Definition 55

Example2: Intelligent Traffic System
KV Query


3000 cross roads

Gateway

DAG

Travel time

Fake Plate

Over speed

Traffic planning

Real time statistics

56

Other demos
· complex Graph · Stock data analysis (Drawdown tracking) · ITS · Scalability, 100 nodes, 1,000,000 tasks · DSL
57

Status & Plan
· Our goal: Make this an Apache project
­ Welcome code contribution! ­ http://gearpump.io
· Plan:
­ Connect: IOT ­ Platform: Dynamic DAG, Exactly-once, etc. (release soon) ­ Data: Real-time analysis algorithm
58

References
·  ReactiveAkka, 20152A · Gearpump whitepaper http://typesafe.com/blog/gearpump-real-time-streaming-engine-using-akka ·  , 201310 · Stonebraker http://cs.brown.edu/~ugur/8rulesSigRec.pdf · https://github.com/intel-hadoop/gearpump · Gearpump: https://github.com/intel-hadoop/gearpump · http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/ · https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-
three-cheap-machines · Sqlstream http://www.sqlstream.com/customers/ · http://www.statsblogs.com/2014/05/19/a-general-introduction-to-stream-processing/ · http://www.statalgo.com/2014/05/28/stream-processing-with-messaging-systems/ · Gartner report on IOT http://www.zdnet.com/article/internet-of-things-devices-will-dwarf-number-
of-pcs-tablets-and-smartphones/
59

60

