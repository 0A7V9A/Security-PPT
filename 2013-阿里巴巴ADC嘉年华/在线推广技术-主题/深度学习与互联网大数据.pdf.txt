
Kai Yu Deputy Managing Director, Baidu IDL

Machine Learning
"...the design and development of algorithms that allow computers to evolve behaviors based on empirical data, ..."

7/23/2013

2

All Machine Learning Models in One Page

Shallow Models
7/23/2013

Deep Models
3

Shallow Models Since Late 80's
Neural Networks Boosting Support Vector Machines Maximum Entropy ...

Given good features, how to do classification?

7/23/2013

4

Since 2000 ­ Learning with Structures
 Kernel Learning  Transfer Learning  Semi-supervised Learning  Manifold Learning  Sparse Learning  Matrix Factorization ...

Given good features, how to do classification?

7/23/2013

5

Mission YeMt Aicncionmgp$lfioshre$dStructure$ Massive$increase$in$both$computa: onal$power$and$the$amount$of$ data$available$from$web,$video$cameras,$laboratory$measurements.$

Images$&$Video$

Text$&$Language$$

Speech$&$Audio$ Gene$Expression$

Product$$ Recommenda: on$

Rela: onal$Data/$$ Social$Network$

Climate$Change$ Geological$Data$

Mostly$Unlabeled$

·$Develop$sta: s: cal$models$that$can$discover$underlying$structure,$cause,$or$

sta: s: cal$correla: on$from$data$in$unsupervised*or$semi,supervised*way.$$

·$Mul: ple$applica: on$domains.$

Slide Courtesy: Russ Salakhutdinov

7/23/2013

6

The pipeline of machine visual perception

Low-level sensing

Preprocessing

Feature extract.

Most Efforts in Machine Learning

Feature selection

Inference: prediction, recognition

· Most critical for accuracy · Account for most of the computation for testing · Most time-consuming in development cycle · Often hand-craft in practice

7/23/2013

7

Computer vision features

SIFT HoG

Spin image
RIFT GLOH
Slide Courtesy: Andrew Ng

Deep Learning: learning features from data

Low-level sensing

Preprocessing

Feature extract.

Machine Learning

Feature selection

Inference: prediction, recognition

Feature Learning

7/23/2013

9

Convolution Neural Networks

Coding

Pooling

Coding

Pooling

Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1989.

7/23/2013

10

"Winter of Neural Networks" Since 90's
Non-convex Need a lot of tricks to play with Hard to do theoretical analysis

7/23/2013

11

The Paradigm of Deep Learning

Deep Learning Since 2006

Neural networks 

7/23/2013

13

Race on ImageNet (Top 5 Hit Rate)
72%, 2010 74%, 2011

7/23/2013

14

The Best system on ImageNet (by 2012.10)

Local Gradients

Pooling

Variants

of Sparse Coding:

e.g, SIFT, HOG

LLC, Super-vector

Pooling

Linear Classifier

- This is a moderate deep model - The first two layers are hand-designed

7/23/2013

15

Challenge to Deep Learners
Key questions:
What if no hand-craft features at all? What if use much deeper neural networks?

7/23/2013

-- By Geoff Hinton
16

Answer from Geoff Hinton 2012.10

72%, 2010 74%, 2011 85%, 2012

7/23/2013

17

The Architecture

7/23/2013

Slide Courtesy: Geoff Hinton
18

The Architecture
­ 7 hidden layers not counting max pooling. ­ Early layers are conv., last two layers globally connected. ­ Uses rectified linear units in every layer. ­ Uses competitive normalization to suppress hidden activities.

7/23/2013

Slide Courtesy: Geoff HInton
19

Revolution on Speech Recognition Word error rates from MSR, IBM, and the Google speech group

7/23/2013

Slide Courtesy: Geoff Hinton
20

Deep Learning for NLP

Natural Language Processing (Almost) from Scratch, Collobert et al,

7/23/2013 JMLR 2011

21

Biological & Theoretical Justification

Why Hierarchy?
Theoretical:
"...well-known depth-breadth tradeoff in circuits design [Hastad 1987]. This suggests many functions can be much more efficiently represented with deeper architectures..." [Bengio & LeCun 2007]
Biological: Visual cortex is hierarchical (Hubel-Wiesel
Model 1981 Nobel Prize)

[Thorpe]

Sparse DBN: Training on face images

object models
object parts (combination of edges)
edges

Deep Architecture in the Brain

Area V4

Higher level visual abstractions

Area V2

Primitive shape detectors

Area V1 Retina

Edge detectors pixels

pixels

Note: Sparsity important for these results
[Lee, Grosse, Ranganath & Ng, 2009]

Sensor representation in the brain

Auditory Cortex

Auditory cortex learns to see.
(Same rewiring process also works for touch/ somatosensory cortex.)

Seeing with your tongue
[Roe et al., 1992; BrainPort; Welsh & Blasch, 1997]

Deep Learning in Industry

Microsoft
 First successful deep learning models for
speech recognition, by MSR in 2010

7/23/2013

27

"Google Brain" Project

Ley by Google fellow Jeff Dean

Published two papers,
ICML2012, NIPS2012

Company-wise large-scale
deep learning infrastructure

7/23/2013

Significant success on
images, speech, ...
28

Deep Learning @ Baidu
 Started working on deep learning in 2012
summer

 Big success in speech recognition and image
recognition, 6 products have been launched so far.

 Recently, very encouraging results on CTR

 Meanwhile, efforts are being tried on in areas
like LTR, NLP...

7/23/2013

29

Deep Learning Ingredients
 Building blocks
­ RBMs, autoencoder neural nets, sparse coding, ...
 Go deeper: Layerwise feature learning
­ Layer-by-layer unsupervised training ­ Layer-by-layer supervised training
 Fine tuning via Backpropogation
­ If data are big enough, direct fine tuning is enough
 Sparsity on hidden layers are often helpful.

7/23/2013

30

Building Blocks of Deep Learning

CVPR 2012 Tutorial Deep Learning for Vision

09.00am: 10.00am: 10.30am: 11.30am: 12.30pm: 01.30pm:
02.30pm:
03.00pm: 03.30pm: 04.00pm: 05.00pm: 05.30pm:

Introduction Coffee Break Sparse Coding Neural Networks Lunch Restricted Boltzmann Machines Deep Boltzmann Machines Coffee Break Transfer Learning Motion & Video Summary / Q & A End

Rob Fergus (NYU)
Kai Yu (Baidu) Marc'Aurelio Ranzato (Google)
Honglak Lee (Michigan) Ruslan Salakhutdinov (Toronto)
Ruslan Salakhutdinov (Toronto) Graham Taylor (Guelph) All

Building Block 1 - RBM

Restricted Boltzmann Machine

7/23/2013

Slide Courtesy: Russ Salakhutdinov 34

Restricted Boltzmann Machine
Restricted:

7/23/2013

Slide Courtesy: Russ Salakhutdinov 35

Model Parameter Learning
i.i.d.

Approximate*maximum*likelihood*learning:**

7/23/2013

Slide Courtesy: Russ Salakhutdinov 36

Contrastive Divergence

7/23/2013

Slide Courtesy: Russ Salakhutdinov 37

RBMs for Images

7/23/2013

Slide Courtesy: Russ Salakhutdinov 38

Layerwise Pre-training

7/23/2013

Slide Courtesy: Russ Salakhutdinov 39

DBNs for MNIST Classification

7/23/2013

Slide Courtesy: Russ Salakhutdinov 40

Deep Autoencoders for Unsupervised Feature Learning

7/23/2013

Slide Courtesy: Russ Salakhutdinov 41

Searching$Large$Image$Database$ Image Retrievuasliunsgin$BgiBnianarryy$CCooddeess$
·$Map$images$into$binary$codes$for$fast$retrieval.*

·$Small$Codes,$Torralba,$Fergus,$Weiss,$CVPR$2008$ ·*Spectral$Hashing,$Y.$Weiss,$A.$Torralba,$R.$Fergus,$NIPS$2008* ·*Kulis$and$Darrell,$NIPS$2009,$Gong$and$Lazebnik,$CVPR$20111$ ·*Norouzi$and$Fleet,$ICML$2011,$*

7/23/2013

Slide Courtesy: Russ Salakhutdinov 42

Building Block 2 ­ Autoencoder Neural Net

AUTO-ENCODERS NEURAL NETS Autoencoder Neural Network

input

code
encoder

prediction
decoder

Error

­ input higher dimensional than code - error: ||prediction - input||2
- training: back-propagation

116
Ranzato

7/23/2013

Slide Courtesy: Marc'Aurelio Ranzato 44

Sparse ASuPtAoRenScEoAdUerTSOpa-rEsNitCyODERS Penalty

input
input

Sparsity
encodePrenaltycode

decoder
prediction

prediction

encoder code

decoder

Err

Error

­ -

es­--prelsraoprsorrasorrs:rs:is:ittu||yym||pppporerefenedasnildqctautyilaic:tor|tnye|i:c-oroe|indnc|peoc-u|n|otsi1|dtn|r2peucu|t|ti1o|n|2error

and

sparsity

- lo- strsa:insinugm: baocfk-spqroupaargaetironeconstruction error anRdanszap1t1ao7 rsity

- training: back-propagation

Ran

7/23/2013

Slide Courtesy: Marc'Aurelio Ranzato 45

SPARSE AUTO-ENCODERS
Sparse ASuPtAoRenScEoAdUerTO-ENCODERS

Sparsity

Sparsity Penalty
Penalty

input

input

encoder

encoder
code

deccooddeer

preddeicctoiodner

prediction

Error

Er

­ -

esprraorrs­:it|iyn|pppreuentda:ilctXtyi:o|nc|c-oodidnepe:u||th1||=2

W

T

X

 -
-

tlorsasin: i-snuglmo: sbosaf:cskq-LpuraorepXarge;actWoionnst=ructWion

ehr-rorXand2sparsity

h 117
j Ranzj ato

Le et al. "ICA with reconstruction cost.." NIPS 2011

Ran

7/23/2013

Slide Courtesy: Marc'Aurelio Ranzato 46

Building Block 3 ­ Sparse Coding

Sparse coding
Sparse coding (Olshausen & Field,1996). Originally developed to explain early visual processing in the brain (edge detection).
Training: given a set of random patches x, learning a dictionary of bases [1, 2, ...]
Coding: for data vector x, solve LASSO to find the sparse coefficient vector a

7/23/2013

48

Sparse coding: training time
Input: Images x1, x2, ..., xm (each in Rd) Learn: Dictionary of bases f1, f2, ..., fk (also Rd).
Alternating optimization: 1.Fix dictionary f1, f2, ..., fk , optimize a (a standard LASSO problem 2.Fix activations a, optimize dictionary f1, f2, ..., fk (a convex QP problem)

Sparse coding: testing time
Input: Novel image patch x (in Rd) and previously learned fi's Output: Representation [ai,1, ai,2, ..., ai,k] of image patch xi.

 0.8 *

+ 0.3 *

+ 0.5 *

Represent xi as: ai = [0, 0, ..., 0, 0.8, 0, ..., 0, 0.3, 0, ..., 0, 0.5, ...]

Sparse coding illustration

Natural Images

50

100

150

200

50

250 100

300 150

350 200

400

250

50

450

300

100

500

50

100

150

200 250 350

300

350

400

415500 500

200 400
250 450
300 500
50 100 150 230500 250 300 350 400 450 500

400

450

500 50 100 150 200 250 300 350 400 450 500

Test example

Learned bases (f1 , ..., f64): "Edges"

 0.8 *

+ 0.3 *

+ 0.5 *

[a1, ..., ax64] = [0, 00,....8, *0, 0.8f, 036, ..., 0,+0.30,.30,*..., 0, 0.f54,20] + 0.5 *
(feaftu63re representation)

Slide credit: Andrew Ng Compact & easily interpretable

RBM & autoencoders

- also involve activation and reconstruction - but have explicit f(x) - not necessarily enforce sparsity on a - but if put sparsity on a, often get improved results
[e.g. sparse RBM, Lee et al. NIPS08]

a
f(x) encoding x

a g(a) decoding
x'

7/23/2013

52

Sparse coding: A broader view

Any feature mapping from x to a, i.e. a = f(x), where
-a is sparse (and often higher dim. than x) -f(x) is nonlinear -reconstruction x'=g(a) , such that x'x

a f(x)
x

a g(a)
x'

Therefore, sparse RBMs, sparse auto-encoder, even VQ can be viewed as a form of sparse coding.

7/23/2013

53

Example of sparse activations (sparse coding)

x1

x3

x2

xm

...

a1 [ 0 | | 0 0 ... 0a2[ | | 0 0 0 ... 0  a3 [ | 0 | 0 0 ... 0 
am [ 0 0 0 | | ... 0 

· different x has different dimensions activated
· locally-shared sparse representation: similar x's tend to have similar non-zero dimensions

7/23/2013

54

Example of sparse activations (sparse coding)

x1 x2

x3 xm

...

a1 [ | | 0 0 0 ... 0a2[ 0 | | 0 0 ... 0  a3 [ 0 0 | | 0 ... 0 
am [ 0 0 0 | | ... 0 

· another example: preserving manifold structure
· more informative in highlighting richer data structures, i.e. clusters, manifolds,

7/23/2013

55

Sparsity vs. Locality

sparse coding local sparse coding
7/23/2013

· Intuition: similar data should get similar activated features
· Local sparse coding: · data in the same neighborhood tend to have shared activated features;
· data in different neighborhoods tend to have different features activated.
56

Sparse coding is not always local: example

Case 1 independent subspaces
· Each basis is a "direction" · Sparsity: each datum is a linear combination of only several bases.
7/23/2013

Case 2 data manifold (or clusters)
· Each basis an "anchor point" · Sparsity: each datum is a linear combination of neighbor anchors. · Sparsity is caused by locality.
57

Two approaches to local sparse coding

Approach 1

Approach 2

Coding via local anchor points Coding via local subspaces

Local coordinate coding
Learning locality-constrained linear coding for image classification, Jingjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas Huang. In CVPR 2010.
Nonlinear learning using local coordinate coding, Kai Yu, Tong Zhang, and Yihong Gong. In NIPS 2009.

Super-vector coding
Image Classification using Super-Vector Coding of Local Image Descriptors, Xi Zhou, Kai Yu, Tong Zhang, and Thomas Huang. In ECCV 2010.
Large-scale Image Classification: Fast Feature Extraction and SVM Training, Yuanqing Lin, Fengjun Lv, Shenghuo Zhu, Ming Yang, Timothee Cour, Kai Yu, LiangLiang Cao, Thomas Huang. In CVPR 2011

7/23/2013

58

Two approaches to local sparse coding

Approach 1

Approach 2

Coding via local anchor points Coding via local subspaces

Local coordinate coding

Super-vector coding

- Sparsity achieved by explicitly ensuring locality - Sound theoretical justifications - Much simpler to implement and compute - Strong empirical success

7/23/2013

59

Hierarchical Sparse Coding
(W ,) =
n

S (W

)



1
n

n

w

iw

T i

R

Yu, Lin, & Lafferity=, 1CVPR 11

W ,

L (W ,)+

1
n

W 1+ 
L (W ,)

 0,

\alpha

··xn ] R d×n B  R d×p

w

x

L (W
1n
n

,) =

1
2n

1
2w

ix i

-

XL (-W ,BW)

2 F

+

2
n

B w i 2 + 2w i ()w i

S

i= 1

 (

W = (w 1 w 2 ···w nW)  R p×n
 R q

S (W W

p ×q

+

7/23/2013

q
()  k

-1
(k ) .60

Hierarchical Sparse Coding on MNIST
Yu, Lin, & Lafferty, CVPR 11
HSC vs. CNN: HSC provide even better performance than CNN
 more amazingly, HSC learns features in unsupervised manner!
61

Second-layer dictionary

Yu, Lin, & Lafferty, CVPR 11

A hidden unit in the second layer is connected to a unit group in the 1st layer: invariance to translation, rotation, and deformation

62

Adaptive Deconvolutional Networks for Mid and High Level Feature Learning
Matthew D. Zeiler, Graham W. Taylor, and Rob Fergus, ICCV 2011

 Hierarchical
Convolutional Sparse

L4 Feature Maps

Coding.

L3 Feature

 Trained with respect

Maps

to image from all

layers (L1-L4).

L2 Feature

 Pooling both spatially

Maps

and amongst features.

 Learns invariant mid-

L1 Feature Maps

level features.

Image

Select L4 Features Select L3 Feature Groups
Select L2 Feature Groups L1 Features

Recap: Deep Learning Ingredients
 Building blocks
­ RBMs, autoencoder neural nets, sparse coding, ...
 Go deeper: Layerwise feature learning
­ Layer-by-layer unsupervised training ­ Layer-by-layer supervised training
 Fine tuning via Backpropogation
­ If data are big enough, direct fine tuning is enough
 Sparsity on hidden layers are often helpful.

7/23/2013

64

Layer-by-layer unsupervised training + fine tuning
Loss = supervised_error + unsupervised_error

input

prediction

label

Weston et al. "Deep learning via semi-supervised embedding" ICML 2008 Ra

7/23/2013

Slide Courtesy: Marc'Aurelio Ranzato 65

Layer-by-Layer Supervised Training

7/23/2013

66

Geoff Hinton, ICASSP 2013

Large-scale training Stochastic vs. batch

The challenge The Challenge
A Large Scale problem has:
­ lots of training samples (>10M) ­ lots of classes (>10K) and ­ lots of input dimensions (>10K).

­ best optimizer in practice is on-line SGD which is naturally
sequential, hard to parallelize.
­ layers cannot be trained independently and in parallel, hard to
distribute
­ model can have lots of parameters that may clog the network,
hard to distribute across machines

7/23/2013

Slide Courtesy: Marc'Aurelio Ranzato 16590

A solution by model parallelism
Our Solution

1st machine

2nd

3rd

machine machine

MODEL PARALLELISM
+

input #3 input #2 input #1

MODEDL ATA PARPAALLREALILSLMELISM

153

Ranzato

Le et al. "Building high-level features using large-scale unsupervised learning" ICML 2012

Ra

7/23/2013

Slide Courtesy: Marc'Aurelio Ranzato 70

input #3 input #3 input #2
input #2input #1
input #1

MODEL PAMRAOLDLEELLISM
+ PARALLELISM
+DATA
PARDAALTLAELISM
PARALLELISM

Le et al. "Building high-level features using large-scale unsupervised learning" ICML 2012

Ra

Le e7t/23a/l2.0"1B3uilding high-level features using large-scSalleiduensCupoeurrvtiesesdy:leMaranirncg'A" uICreMlLio2R01a2nzato 7R1an

Asynchronous SGD
Asynchronous SGD
PARAMETER SERVER
L 1

1st replica
7/23/2013

2nd replica

3rd replica 157

Ranzato
Slide Courtesy: Marc'Aurelio Ranzato 72

Asynchronous SGD
Asynchronous SGD
PARAMETER SERVER
1

1st replica
7/23/2013

2nd replica

3rd replica 158

Ranzato
Slide Courtesy: Marc'Aurelio Ranzato 73

PARAMETER SERVER
L 2

1st replica
7/23/2013

2nd replica

3rd replica 160

Ranzato
Slide Courtesy: Marc'Aurelio Ranzato 74

PARAMETER SERVER
2

1st replica
7/23/2013

2nd replica

3rd replica 161

Slide Courtesy: Marc'Aurelio RanRzaatnoza7t5o

Unsupervised Learning With 1B Paramete
Training A Model with 1 B Parameters
Deep Net:
­ 3 stages
­ each stage consists of local filtering, L2 pooling, LCN - 18x18 filters - 8 filters at each location - L2 pooling and LCN over 5x5 neighborhoods
­ training jointly the three layers by: - reconstructing the input of each layer - sparsity on the code

7/23/2013

Slide Courtesy: Marc'Aurelio Ranzato 76

Conclusion Remark
 We see the promise of deep learning  Big success on speech, vision, and many other  Large-scale training is the key challenge  More successful stories are coming ...

7/23/2013

77

