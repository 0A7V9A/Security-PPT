#RSAC

SESSION ID: MLAI-M06
It's Not Fair! Detecting Algorithmic Bias with Open Source Tools

Mike Kiser
Director of Strategy and Standards SailPoint @_MikeKiser

Mo Badawy
Principal Data Scientist SailPoint

#RSAC
Disclaimer
Presentations are intended for educational purposes only and do not replace independent professional judgment. Statements of fact and opinions expressed are those of the presenters individually and, unless expressly stated to the contrary, are not the opinion or position of RSA Conference LLC or any other cosponsors. RSA Conference does not endorse or approve, and assumes no responsibility for, the content, accuracy or completeness of the information presented. Attendees should note that sessions may be audio- or video-recorded and may be published in various media, including print, audio and video formats without further notice. The presentation template and any media capture are subject to copyright protection.
©2022 RSA Conference LLC or its affiliates. The RSA Conference logo and other trademarks are proprietary. All rights reserved.
2

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved.

3

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved.

4

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved.

5

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved.

6

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved.

7

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved.

8

#RSAC

Transparency

Fairness

#RSAC

Transparency

Fairness

#RSAC
Why are bubbles round?
Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 11

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 12

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 13

#RSAC
Impact of Transparency
Safety
Model Adjustment
Objective Assessment

#RSAC
Model Interpretability
https://xkcd.com/1725/

#RSAC
COMPAS Model Interpretability

#RSAC
Model Agnostic Methods
Global vs Local LIME - Local Interpretable Model-Agnostic Explanations
SHAP - SHapley Additive exPlanations

#RSAC
Comparison of Transparency Tools

Local Interpretable Model-agnostic Explanations
(LIME)
· Local Model Approximation
· Less Intuitive to Human Reasoning
· Relatively Fast

· Game Theory
· More Intuitive to Human Reasoning
· Relatively Slow (Faster on tree structures, slow on k-nearest neighbor)

#RSAC
Global vs Local Interpretation
- Global interpretation: general landscape of features/attributes - Local interpretation: a particular instance or a small set of instances.

#RSAC
Diabetes Progression Dataset

Prediction Target:

Quantitative measure of disease progress

Instances:

442

Attributes:

Age Sex Body Mass Index (BMI) Blood Pressure (BP) LDL HDL ...

https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/diabetes.html

#RSAC
LIME Results: Diabetes Dataset

Negative

Positive

· Predicted value is a representation of diabetes progression
· Features are identified by their relative impact

#RSAC
SHAP Results: Diabetes Dataset
· Shapley value in game theory · Shows change in prediction value by feature

Google's What-If Tool

#RSAC
Data Exploration "Counter Factuals" Fairness
Supported Models: Binary classification Mutli-class classification Regression tasks

#RSAC

Transparency

Fairness

#RSAC
Bias
Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 25

Shortcut

#RSAC
Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 26

#RSAC
Shortcut Impartiality
Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 27

#RSAC
Shortcut Impartiality Self-Interest
Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 28

#RSAC
Gender Bias in German Credit Dataset

Financial

· Gender part of original data
· Checking for bias is relatively straightforward
· Gender is relatively low impact

Intention Gender
Relative Impact of Attributes on Credit

Hidden Bias: Renal Failure Dataset

#RSAC
· Racial bias not immediately apparent
· Algorithm was optimized for cost

#RSAC
Aequitas Fairness Toolkit
http://aequitas.dssg.io/

#RSAC
Aequitas Result Set

#RSAC
Aequitas Fairness Decision Tree

#RSAC
Aequitas: Formatting the Data Set
score The predicted outcome label_value Ground truth (the actual result) attribute_n Attribute within the model

#RSAC
Aequitas Demo
Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 35

Comparison of Fairness Tools

Fairness Comparison AI Fairness 360
Audit-AI Thesis-ML Aequitas FairML FairTest Themis

Benchmarks fairness of algorithms
Embeddable code for bias/mitigation Developer-focused Input cleansing Equal Employment Opportunity Commission Standards Fairness in hiring driven
Fairness-aware ML algorithms
Toolkit (libraries, command line, web interface) Usable by policymakers and auditors Calculates attribute significance for black-box models
Subgroup / Decision Tree created from user population Reports on subgroup / feature association for potential bias Python module for statistical analysis of bias

Fairness Measures

Top K ranking algorithms

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 36

#RSAC
So, What Now?
Next week:
­ Inventory and consider the use cases of machine learning within your organization (both internal and outsourced)
­ Discuss with IRB, etc about potential issues that might need to be addressed
In three months:
­ Investigate the tools that might be applicable for your datasets and machine learning usage:
Transparency: LIME / SHAP Fairness: (Various tools)
Within six months:
­ Begin to implement these tools and seek to rectify issues that may present themselves
37

Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 38

Links to Various Tools

Transparency

Lime SHAP Google What-if?

https://github.com/marcotcr/lime https://github.com/slundberg/shap https://pair-code.github.io/what-if-tool/

Fairness

Aequitas Fairness Comparison AI Fairness 360 Audit-AI Thesis-ML FairML FairTest Themis Fairness Measures

https://github.com/dssg/aequitas https://github.com/algofairness/fairness-comparison https://github.com/IBM/aif360 https://github.com/pymetrics/audit-ai https://github.com/cosmicBboy/themis-ml https://github.com/adebayoj/fairml https://github.com/columbia/fairtest https://themis-ml.readthedocs.io/en/latest/index.html https://fairnessmeasures.github.io/

Explore, Explain and Examine Predictive Models: https://pbiecek.github.io/ema/
Copyright © SailPoint Technologies Holdings, Inc. 2021. All rights reserved. 39

