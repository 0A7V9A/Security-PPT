CloudLeak: DNN Model Extractions from Commercial
MLaaS Platforms
Yier Jin, Honggang Yu, and Tsung-Yi Ho University of Florida, National Tsing Hua University
yier.jin@ece.ufl.edu
#BHUSA @BLACKHATEVENTS

Who We Are
Yier Jin, PhD @jinyier Associate Professor and Endowed IoT Term Professor The Warren B. Nelms Institute for the Connected World Department of Electrical and Computer Engineering University of Florida
Research Interests: · Hardware and Circuit Security · Internet of Things (IoT) and Cyber-Physical System (CPS) Design · Functional programming and trusted IP cores · Autonomous system security and resilience · Trusted and resilient high-performance computing
3

Who We Are
Tsung-Yi Ho, PhD @TsungYiHo1 Professor Department of Computer Science National Tsing Hua University
Program Director AI Innovation Program Ministry of Science and Technology, Taiwan
Research Interests: · Hardware and Circuit Security · Trustworthy AI · Design Automation for Emerging Technologies
4

Who We Are
Honggang Yu @yuhonggang Visiting PhD. student The Warren B. Nelms Institute for the Connected World Department of Electrical and Computer Engineering University of Florida
Research Interests: · Intersection of security, privacy, and machine learning · Embedded systems security · Internet of Things (IoT) security · Machine learning and deep learning with applications in VLSI
computer aided design (CAD)
5

Outline
Background and Motivation
 AI Interface API in Cloud  Existing Attacks and Defenses
Adversarial Examples based Model Stealing
 Adversarial Examples  Adversarial Active Learning  FeatureFool  MLaaS Model Stealing Attacks
Case Study
 Commercial APIs hosted by Microsoft, Face++, IBM, Google and Clarifai
Defenses Conclusions
6

Success of DNN
"Perceptron" "Multi-Layer Perceptron" "Deep Convolutional Neural Network"

DNN based systems are widely used in various applications:

# Parameters # Layers

1E+10 100000000
1000000 10000 100 1

61M
8 AlexNet

Revolution of DNN Struture
7M 138M

16

22

VGG-16 Parameters

GoogLeNet Layers

152 160 60M
120
80
40
0
ResNet-152

7

Commercialized DNN
Machine Learning as a Service (MLaaS)
 Google Cloud Platform, IBM Watson Visual Recognition, and Microsoft Azure
Intelligent Computing System (ICS)
 TensorFlow Lite, Pixel Visual Core (in Pixel 2), and Nvidia Jetson TX
8

Machine Learning as a Service

Prediction API Inputs Outputs User

Black-box
Goal 1: Rich Prediction API

Training API Sensitive Data

Goal 2: Model Confidentiality

Dataset

Suppliers

$$$ per query
Overview of MLaaS Working Flow
9

Machine Learning as a Service

Services
Microsoft
Face++ IBM
Google Clarifai

Products and Solutions Custom Vision Custom Vision
Emotion Recognition API Watson Visual Recognition
AutoML Vision Not Safe for Work (NSFW)

Customization
  ×   ×

Function
Traffic Recognition Flower Recognition
Face Emotion Verification Face Recognition
Flower Recognition Offensive Content
Moderation

Black-box
     

Model Types
NN NN
NN
NN NN
NN

Monetize
 

Confidence Scores



















10

Model Stealing Attacks
Various model stealing attacks have been developed
None of them can achieve a good tradeoffs among query counts, accuracy, cost, etc.

Proposed Attacks F. Tramer (USENIX'16)
Juuti (EuroS&P'19) Correia-Silva (IJCNN'18) Papernot (AsiaCCS'17)

Parameter Size ~ 45k ~ 10M ~ 200M ~ 100M

Queries ~ 102k ~ 111k ~ 66k
~ 7k

Accuracy High High High Low

Black-box?
   

Stealing Cost Low High -

11

Adversarial Example based Model Stealing
12

Adversarial Examples in DNN
Adversarial examples are model inputs generated by an adversary to fool deep learning models.

"source example"

"adversarial perturbation" "advesarial example"

"target label"

+

=

=

Goodfellow et al, 2014

13

Adversarial Examples
Non-Feature-based
 Projected Gradient Descent (PGD) attack  C&W Attack

Source Source

Adversarial Adversarial

Feature-based
 Feature adversary attack  FeatureFool
Source Perturbation Guide

Adversarial

Carlini et al, 2017
Source Perturbation Guide Adversarial

Sabour et al, 2016
14

A Simplified View of Adversarial Examples
f (x)  0
Source example Medium-confidence legitimate example Minimum-confidence legitimate example Minimum-confidence adversarial example Medium-confidence adversarial example Maximum-confidence adversarial example
f (x)  0
A high-level illustration of the adversarial example generation
15

Adversarial Active Learning

We gather a set of "useful examples" to train a substitute model with the performance similar to the black-box model.

f (x)  0

Source example Medium-confidence legitimate example Medium-confidence adversarial example Maximum-confidence adversarial example Minimum-confidence legitimate example Minimum-confidence adversarial example

f (x)  0

"Useful examples"

Illustration of the margin-based uncertainty sampling strategy.
16

FeatureFool: Margin-based Adversarial Examples

To reduce the scale of the perturbation, we further propose a feature-based

attack to generate more robust adversarial examples.

 Attack goal: Low confidence score for true class (we use  to control the confidence

score).

minimize  ,  +   , 

such that   [0,1]

For the triplet loss ,  , we formally define it as:
,  = max(   ,   -    ,   + , 0)

 In order to solve the reformulated optimization problem above, we apply the boxconstrained L-BFGS for finding a minimum of the loss function.

17

FeatureFool: A New Adversarial Attack

(a) Source image (b) Adversarial perturbation

+





(c) Guide Image

(d) Feature Extractor

(e) Salient Features
( + )
()

(f) Box-constrained L-BFGS

(1) Input an image and extract the corresponding n-th layer feature mapping using the feature extractor (a)-(d); (2) Compute the class salience map to decide which points of feature mapping should be modified (e); (3) Search for the minimum perturbation that satisfies the optimization formula (f).

18

FeatureFool: A New Adversarial Attack

Source

Guide Adversarial

Source Guide Adversarial

Source Guide Adversarial

Neutral: 0.99 

Happy: 0.98 

Happy: 0.01 ×

19

MLaaS Model Stealing Attacks

Our attack approach

 Use all adversarial examples to generate the malicious inputs;  Obtain input-output pairs by querying black-box APIs with malicious inputs;  Retrain the substitute models which are generally chosen from candidate Model Zoo.

MLaaS

Inputs

Adversary

Search

Candidate Library
Model Zoo (AlexNet, VGGNet,
ResNet)

Outputs

Malicious Examples (PGD, C&W, FeatureFool)

Illustration of the proposed MLaaS model stealing attacks
20

MLaaS Model Stealing Attacks

Overview of the transfer framework for the model theft attack

(a) Unlabeled Synthetic Datatset

Genuine Domain

(b) MLaaS Query

(c) Synthetic Dataset with Stolen Labels

(d) Feature Transfer

Label
DB
Malicious Domain

(e) Prediction
?

Boundary

Reused Layers

Retrained Layers

Layer copied from Teacher

Layer trained by Student (Adversary)

(1) Generate unlabeled dataset (2) Query MLaaS (3) Use transfer learning method to retrain the substitute model

21

Example: Emotion Classification
Procedure to extract a copy of the Emotion Classification model
1) Choose a more complex/relevant network, e.g., VGGFace
2) Generate/Collect images relevant to the classification problem in source domain and in problem domain (relevant queries)
3) MLaaS query 4) Local model training based on the
cloud query results
Architecture Choice for stealing Face++ Emotion Classification API (A = 0.68k; B = 1.36k; C = 2.00k)
22

Experimental Results

Adversarial perturbations result in a more successful transfer set.

In most cases, our FeatureFool method achieves the same level of accuracy with fewer queries than other methods

Service Model

Microsoft

Traffic

Flower

Queries 0.43k 1.29k 2.15k 0.51k 1.53k 2.55k

RS 10.21% 45.30% 70.03% 26.27% 64.02% 79.22%

Dataset

PGD

CW

10.49% 12.10%

59.91% 61.25%

72.20% 74.94%

27.84% 29.41%

68.14% 69.22%

83.24% 89.20%

FA 11.64% 49.25% 71.30% 28.14% 68.63% 84.12%

FF 15.96% 66.91% 76.05% 31.86% 72.35% 88.14%

Price ($)
0.43 1.29 2.15 1.53 4.59 7.65

Comparison of performance on the victim model (Microsoft) and their local substitute models.

23

Comparison with Existing Attacks
Our attack framework can steal large-scale deep learning models with high accuracy, few queries and low costs simultaneously.
The same trend appears while we use different transfer architectures to steal black-box target model.

Proposed Attacks F. Tramer (USENIX'16)
Juuti (EuroS&P'19) Correia-Silva (IJCNN'18) Papernot (AsiaCCS'17)
Our Method

Parameter Size

Queries

Accuracy

~ 45k

~ 102k

High

~10M

~ 111k

High

~ 200M

~66k

High

~ 100M

~7k

Low

~ 200M

~3k

High

A Comparison to prior works.

Black-box?
    

Stealing Cost Low High Low

24

Evading Defenses

Evasion of PRADA Detection
 Our attacks can easily bypass the defense by carefully selecting the parameter M from 0.1  to 0.8 .
 Other types of adversarial attacks can also bypass the PRADA defense if  is small.

Model ( value)
Traffic ( = 0.92) Traffic ( = 0.97) Flower ( = 0.87) Flower ( = 0.90) Flower ( = 0.94)

PGD
missed 110 110 110 110

CW
missed 110
missed 340 340

Queries made until detection

FF FA
 = 0.8  = 0.5

missed missed

150

110

110

110

220

missed

290

220

350

120

220

350

120

 = 0.1 130 110 140 130 130

25

Conclusion
Black-box MLaaS model stealing is possible and cheap Protection and security should be considered in AI applications Future work will focus on AI chip and AI accelerators
27

Thanks!
Yier Jin @jinyier yier.jin@ece.ufl.edu
28

