SESSION ID: MLAI-W12
Build Intelligent Vulnerability Scoring to Optimize Security Residual Risks

Bill Chen
Chief Security Architect

Gyan Prakash
Chief Security Architect

#RSAC

#RSAC
Agenda
1 Ambiguity Effect ­ Risk Categories and Scope 2 Observations on Attack Pivot Patterns 3 Risk Anatomy - Where It Fails 4 Back to Simplicity 5 Existing Vulnerability Scoring Systems 6 Next Gen Intelligent Risk Management
7 Transforming Risk Management

#RSAC
Ambiguity Effect ­ Risk Categories & Scope
Unknown unknowns go wrong - False Negatives
We even don't know what's going on.
Known unknowns go wrong ­ Zero days.
We don't know it, but we plan for it.
Today's Focus Known knowns go wrong ­ Findings
We know it, and we know how to fix it.
"Anything that can go wrong will go wrong." - Murphy's Law

2018 - Successful Attack Pivot Patterns*
Over 53,000 incidents and 2,216 confirmed data breaches in 2018
48%
Hacks/OWASP Top 10
30%
Malware/Bots
17%
Errors
17%
Social Attacks
12%
Privilege Misuse
11%
Physical Attacks
* Source: Verizon Data Breach Report, 2018.

#RSAC

Anatomy of Risk

Complexities

Counter Measures

Known Vulnerabilities

Compliance Violations

#RSAC
Application Criticality

Anatomy of Risk
Privilege access usages
monitoring
Complexities

Application exploit patterns changes hourly
Counter Measures

Known Vulnerabilities

Compliance Violations

#RSAC
Application Criticality

Anatomy of Risk

Privilege access usages
monitoring
Complexities How effective How effective

Application exploit patterns changes hourly
Counter Measures

are counter

are counter

Measures??

Measures??

Known

Compliance

Vulnerabilities

Violations

#RSAC
Application Criticality

Subjective Ambiguity

Privilege access usages
monitCoorimngplexities
How effective Known are counter VulnerabilitieMs easures??

Application exploit patteCronusnter changes houMrleyasures
How effective are counter ComplMianecaesures?? Violations

#RSAC
Application Criticality

Subjective to Objective

Known Vulnerabilities
Static Code Testing Dynamic Testing
Design Vuln. Pen Testing

Risk = Likelihood x Size of Loss
 =     Compliance Violations  (Config Violations)

Total known Vulnerability

 =   =

Size of Software

# Failed Applications Attacks 24 hrs RTv = RunTime Vuln. = Total Traffic Volume in m per 24 hrs.

#RSAC
Known Unknown
Open Source Vuln NW & Infra Scans Configurations Scans Daily Attack Pattern

Failed Complaince Requirements Compliance Violations = Total Compliance Requirements

Configurations Violations

Ops Violations =

Total Servers

#RSAC
Back to Simplicity
It's all about prioritization.
It is about sorting a list of findings. In the end of the day, it is all about the ability to compare the risk of any two vulnerabilities.

#RSAC
Existing Vulnerability Scoring Systems
Common Weakness Scoring System (CWSS)? Very Similar!
* https://www.first.org/cvss

Next Gen Intelligent Risk Management

Known Vulnerabilities
Static Code Testing Dynamic Testing
Design Vuln. Pen Testing
Known Unknown
Open Source Vuln NW & Infra Scans Configurations Scans

Asset Information Database

Smart Risk Engine

Intelligent Vulnerability
Scoring Systems

Automation Mixer

Supervised Training

Compensation Controls

Daily Attack Pattern

Key Stakeholders Review/Approval

#RSAC
Final Risk Rating

Bayesian/Neural Networks for Vulnerability Scoring

Attack Vector AV = Network
Privileges Required PR = Non
Availability A = Medium Confidentiality
C = High
Personal Card Number Volume
Personal Identity Info Volume
Number of App Users Volume
Counter Measure CM = No
Other factors ...

Service Interruption Partial
PCI Trigger Y/N
GDPR Trigger Y/N
Reported on News Y/N
Loss of Productivity High

Revenue Impact Level = 1
Regulatory Penalty Level = 2
Brand Impact Level = 2

Risk Score Critical

#RSAC

#RSAC
Cost Function
· Prioritizing is a sorting problem

· Pairwise comparison from Expert Opinion to Model Prediction *

Model Opinion

Model Opinion agrees with Expert Opinion

Vuln j

X

If Expert says Vuln i is more sever than Vuln j, but The prediction model says the reverse, then it is counted as a clash

Model Performance

Accuracy of prediction = Number of agreements/Total Number of comparisons

In the sampled training set.

* Bill Chen, "Software Security Economics and Threat Modeling Based on Attack Path Analysis", PhD Dissertation, USC, 2007

Vuln i

Expert Opinion

#RSAC
Estimate ROI of Security Investment with Result Chain

CVE 1 CVE 2 CVE 3

POD 1 POD 2

... ...
Patch, or Encrypt, or Deploy Firewall?
CVE N AV=L, C=H

... ... POD M

Online Retail Customer
Relationship Marketing
Engineering
IT

Revenue Brand
Productivity Regulatory

#RSAC
Transforming Risk Management

Today

Subjective Non repeatable

Next Gen Risk Management Objective
Repeatable

Lack of technical traceability

Exactly traceable to specific vulnerabilities

Focus on rating every finding

Focus on scoring model training & calibration

One rating fits a year

Real time risk profile based on findings, alerts, and mitigation implementation status

#RSAC
Q&A

