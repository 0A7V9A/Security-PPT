SESSION ID: GRC-W10
Barney Fife Metrics: The Bullet That we Have but Don't Use, and Why

Jon Boyens
Manager, Security Engineering and Risk Management Group
National Institute of Standards and Technology

Celia Paulsen
Cybersecurity Researcher National Institute of Standards and
Technology

#RSAC

#RSAC
Disclaimer
The identification of any commercial product or trade name is included solely for the purpose of providing examples of publiclydisclosed events, and does not imply any particular position by the National Institute of Standards and Technology.
2

#RSAC
What's a Barney Fife?

And what does he have to do with metrics?
3

Image courtesy of Rogers and Cowan, Beverly Hills [Public domain], via Wikimedia Commons

#RSAC
Question: it's been 20+ years: is there truly a lack of consensus and progress towards information security metrics maturity? If so, why?
The Bullet
Expected:
­ Industry shared solution(s) ­ Common set of practices
Compare to financial or safety metrics

#RSAC
What we did
Literature:
­ 160 Sources reviewed: 52 chosen
o 28 on information security; 24 on related disciplines
­ 22 described metrics in enough detail
o 429 total metrics; 373 unique metrics (87%)
Interviews
­ 13 subject matter experts
o Diversity in size, sector, regulatory oversight, etc o Senior information security & IT leaders (CIO, CISO, CSO, etc.)

#RSAC
Agenda
State of maturity for cybersecurity metrics
­ Literature vs. Interviews
Challenges to achieving maturity
­ Literature vs. Interviews
Possible Best Practices
­ Literature vs. Interviews
Where we go from here

Categories of Information Security Metrics in Literature

General Risk/Forecasting Incident/Incident Response
Policy Management Financial
Quality Control Personnel/Training Process Management Configuration/Maintenance
Test/Audit Access Control
Network Supply Chain
Maturity Other
0

46

45

36

35

34

28

27

22

19

19

18

15

12

6

5

10

15

20

25

30

35

40

45

#RSAC 50

#RSAC
The State of Maturity of Cybersecurity Metrics
Strategic (27%), managerial (53%), operational (20%)
There is (still) a gap in the quality of managerial metrics (especially middle-management)
Many programs seem to collect similar data, but mature programs use tailored metrics
Mature programs evolve from measuring risk as a single metric to dynamically supporting business / project decisions

#RSAC
Questions Security Professionals Need to Answer
What does good information security look like? What is the vocabulary we need to use to measure information security? How do we relate things to risk and to each other?

Challenges, Best Practices, and What's Next

#RSAC
Challenge #1: Too much data
Early metrics programs collect and report fewer metrics
­ One-size-fits-all does not work ­ Number and frequency of reports decreased as programs matured
Automation = more data (not necessarily better data)
­ Security tools ­ ERM / connected tools ­ Custom tools
Filtering out the noise

#RSAC
Challenge #2: Simple vs. Complex
Quality is more important than quantity Quality = (1) Easy to collect, (2) repeatable, (3) have value
­ Similar to literature
Simple metrics are often not useful
­ e.g. Number of incidents
Complex metrics are difficult to compile & make accurate
­ e.g. financial impact of risk

#RSAC
Challenge #3: Time & Commitment
Finding a metrics champion
­ Must be able to translate board-level needs and priorities to data ­ Must be able to show how data can be useful in their world
Even with executive support, resources, and talented leadership, takes time (e.g. six months for one organization) No such thing as "done"

#RSAC
Best Practice #1: Invest in a Program to Manage Metrics
Metrics Champion
­ Understanding of data science & the business
Make it a business priority
­ Make a program specifically for:
o Identifying metrics o Testing metrics o Delivering metrics / reports o Improving metrics

#RSAC
Best Practice #2: Metrics Should "Tell a Story"
Metrics tied to a goal
­ e.g. "security control effectiveness"; " time" ­ Best if goal is strategic; if the metric can be used to direct strategic
decision-making
Measure trends and improvement, not numbers
­ More data does not necessarily mean better data ­ Show the meaning behind the number
Need agreement on what a metric means and what action should they elicit

#RSAC
Best Practice #3: Experiment
Metrics programs need to be highly adaptive and dynamic
­ Rapidly evolving threats ­ Diverse and changing tools market ­ Changes in leadership priorities
Different people need different information Start small Measure the effectiveness of your measurements!

#RSAC
Early vs. Mature Metrics Programs

Early Programs Use only pre-packaged metrics Everybody receives the same metrics Reporting the same info many times Reporting more often Using simple metrics Operational metrics only Create metrics once Metrics as an after-thought

Mature Programs Build on pre-packaged metrics Metrics customized to function Reporting changes & anomalies Reporting less often Using complex metrics Operational, managerial, & strategic Constantly improving metrics Metrics as key strategy driver

17

#RSAC
Research Needed
Useful executive-level metrics KSAs for driving metrics programs Case studies:
­ Example metrics ­ How metrics are tested
How to tailor information security metrics

#RSAC
The bullet we have but don't use
"Data! Data! Data! I can't make bricks without clay!"
­ Sir Arthur Conan Doyle's Sherlock Holmes

#RSAC
Apply What You Have Learned Today
Next week:
­ Identify a metrics champion for your organization ­ NIST SP 800-55 Rev. 1, Performance Measurement Guide for Information Security
In the next three months:
­ Make a list of the data sources you have access to ­ Measure the effectiveness of your current metrics ­ Define what stories you need to hear/tell
Within six months:
­ Have a core set of metrics for each layer of the organization ­ Create a program to test-drive metrics ­ Share your results!
20

Questions???

Jon Boyens
jon.boyens@nist.gov

scrm@nist.gov

Celia Paulsen
celia.paulsen@nist.gov

