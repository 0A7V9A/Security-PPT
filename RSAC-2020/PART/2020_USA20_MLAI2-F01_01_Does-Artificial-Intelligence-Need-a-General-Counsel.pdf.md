SESSION ID: MLAI2-F01
Does Artificial Intelligence Need a General Counsel?

Alan Brill
Senior Managing Director Kroll Cyber Risk

Stacy Scott
Managing Director Kroll Cyber Risk

#RSAC

#RSAC
Agenda

Where Is Artificial Intelligence (AI) Used In Business?

AI's Popularity

What Could & Has Gone
Wrong?

How To Course Correct

Key Takeaways

#RSAC
Where Is AI Used in Business?

#RSAC
And Its Use is Only Getting Bigger...
By 2030, the average simulation shows that some 70% of companies might have adopted at least one type of AI technology.1
The AI market will grow to a $190 billion industry by 2025.2
83% of businesses say AI is a strategic priority for their businesses today.3
1 mckinsey.com 2 marketsandmarkets.com 3 forbes.com

#RSAC
A Starting Point...Why Build AI?
· Traditional computer programming follow defined rules (i.e. it does what it does today and will do the BECAUSE same tomorrow)
· Basic Tenet of Compliance and testing
· Corporate counsel believe they are tech savvy but acknowledge their comfort level and confidence with technology have limitations, specifically around artificial intelligence (AI).

#RSAC
A Starting Point...Why Build AI?
· "Everything we love about civilization is a product of intelligence, so amplifying our human intelligence with artificial intelligence has the potential of helping civilization flourish like never before ­ as long as
AND we manage to keep the technology beneficial."
· - Max Tegmark

A Starting Point...Why Build AI?
One of the Goals of AI & Deep Learning (DL) =
Avoid Avoidable Problems

#RSAC
THEN
· AI rules will evolve and rules will change
· We should NOT build AI to be able to say we built it, or we have them
· The competition has one, so we need one too ­ and right now schedule the AI/DL timeline
· But rather, because we want to avoid avoidable problems and AI technology should be fair

#RSAC
What Could & Has Gone Wrong?

Autonomous weapons: AI programmed
to do something dangerous

Social manipulation: Social media
through its autonomous-
powered algorithms is very effective
at target marketing

Invasion of privacy & social grading: It is now possible to track & analyze an individual's every move
online

Misalignment between our goals and the
machine's

Discrimination

Phalanx CIWS
8

#RSAC
What Could & Has Gone Wrong?
Consider a U.S. AI/Deep Learning System Used by a Financial Services Company to Make Decisions
About Loans....
AI System "training" provided 250,000 records of prior loans AI software analyzes factors related to the likelihood of successful repayment

What Could & Has Gone Wrong?
Consider a U.S. AI/Deep Learning System Used by a Financial Services Company to Make Decisions
About Loans....
One learned approval/denial factor is postal code AI now places greater weight in loan approval/denial based on where the borrower lives

#RSAC
POSTAL CODE OUTLINED IN RED STATISTICALLY LESS LIKELY TO REPAY A LOAN TO THE BANK

#RSAC
What Could & Has Gone Wrong?
The Community Reinvestment Act of 1977 made it illegal to base lending
decisions on the neighborhood where a person lives.
Without it, financial institutions literally drew red lines on maps around minority communities and denied them access to services.

#RSAC
Why does this happen?
AI Experts
Organization may hire AI experts but not necessarily subject matter experts.

#RSAC
Why does this happen?

Tech Focus

Focused on building (e.g., make loan decisions) & never think about legal or regulatory issues

Why does this happen?

#RSAC
Who's Going To Tell Them?
· Can you afford to hope that they figure this out or research it themselves?

#RSAC
But Wait...There's Another Danger
Bias can creep in at many stages of the deep learning process, and the standard practices in computer science aren't designed to detect it.
MIT Technology Review in February 2019

An example of implicit bias...
A facial recognition system used by police was 99% accurate with white males. It was substantially less accurate in identifying women or people of color.
This turned out to be because the enormous training set used for the system consisted mostly of photographs of white males.

#RSAC
This Photo by Unknown Author is licensed under CC BY

#RSAC
Another example of implicit bias...

This Photo by Unknown Author is licensed under CC BY-SA-NC

According to Reuters, Amazon stopped using a new AI system that reviewed applicant's resumes looking for top-level talent. But the system was trained on resumes submitted for 10 years, and most of those were from men.
The AI system penalized resumes that included the word "women's". It also downgraded graduates of two allwomen's colleges.

#RSAC
How Do We Course Correct on AI Systems?

1. Get These Experts Involved in Defining & Building AI

LEGAL

COMPLIANCE &
REGULATORY

2. Collect larger & more diverse data sets to use for AI training

#RSAC
How Do We Course Correct on AI Systems?

Get Legal, Compliance, & Regulatory Experts Involved
in Defining & Building AI

AI System's Technical Capabilities (What it
Could Do)

AI System Context

#RSAC
How Do We Course Correct on AI Systems?

Get Legal, Compliance, & Regulatory Experts Involved
in Defining & Building AI
Part of an AI System's Context is the
Legal/Regulatory/Contractual Framework Within Which it
Operates

AI System Context

There are specific legal/reg./contract provisions that limit the valid actions of an AI
system.

#RSAC
How Do We Course Correct on AI Systems?

Get Legal, Compliance, & Regulatory Experts Involved
in Defining & Building AI

Think of the legal/reg/contract rules as a fence that should limit the freedom of action of
the AI system.

SAFE

AI System

Context

UNSAFE

QUESTIONABLE

#RSAC
How Do We Course Correct AI Systems?

Get Legal, Compliance, & Regulatory Experts Involved in Defining & Building AI
Collect larger & more diverse data sets to use for AI training

IMPLICIT BIAS
First, if you don't know about it, you're unlikely to prevent it.

PREVENTING IT REQUIRES
THINKING ABOUT
· How you frame the problem (what constitutes "success"?)
· How you collect data (does the data represent reality? Does it reflect existing/past prejudices?
· How was the data prepared? What attributes are provided to the AI system to use?

#RSAC
How Do We Course Correct AI Systems?
Dealing with implicit bias can be harder than you think...

Key Takeaways
AI/DL systems being built in private & public sector systems
AI built by Computer/AI/Data Science Specialist with potential for no knowledge of legal & social science areas

#RSAC
AI IS GROWING
BRING IN LEGAL/ COMPLIANCE EXPERTS

Key Takeaways
Identifying a problem after the system is launched may be too late to avoid consequences.
AI training data sets can include bias due to being uncomprehensive, incomplete, and too small.

#RSAC
IDENTIFY PROBLEMS DURING DESIGN &
BUILD
USE LARGER, MORE DIVERSE,
& MULTIPLIE TRAINING DATA
SETS

Thank you for inviting us. If we can help, please

#RSAC

contact us.

abrill@kroll.com
stacy.scott@kroll.com
If you would like to receive our cyberdefense or intel threat newsletters or our reports on fraud or other subjects, just let us know.

